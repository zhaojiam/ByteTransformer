In file included from /export/users/zhaojiam/src/ByteTransformer/bytetransformer/th_op/ths_op_f.cc:21:
In file included from /export/users/zhaojiam/src/ByteTransformer/bytetransformer/th_op/bert_transformer_ext.h:22:
In file included from /export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/bert_transformer.h:25:
In file included from /export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/cutlass_attention.h:25:
/export/users/zhaojiam/src/ByteTransformer/cutlass_contrib/include/cutlass/contrib/args_pack_def.h:28:83: error: unknown type name 'nullptr_t'; did you mean 'platform::nullptr_t'?
   28 | template <typename Element_, typename Layout_, int Alignment, typename ParamOp_ = nullptr_t>
      |                                                                                   ^~~~~~~~~
      |                                                                                   platform::nullptr_t
/export/users/zhaojiam/src/ByteTransformer/3rdparty/cutlass/include/cutlass/platform/platform.h:330:12: note: 'platform::nullptr_t' declared here
  330 | using std::nullptr_t;
      |            ^
In file included from /export/users/zhaojiam/src/ByteTransformer/bytetransformer/th_op/ths_op.cc:21:
In file included from /export/users/zhaojiam/src/ByteTransformer/bytetransformer/th_op/ths_op.h:22:
In file included from /export/users/zhaojiam/src/ByteTransformer/bytetransformer/th_op/bert_transformer_ext.h:22:
In file included from /export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/bert_transformer.h:25:
In file included from /export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/cutlass_attention.h:25:
/export/users/zhaojiam/src/ByteTransformer/cutlass_contrib/include/cutlass/contrib/args_pack_def.h:28:83: error: unknown type name 'nullptr_t'; did you mean 'platform::nullptr_t'?
   28 | template <typename Element_, typename Layout_, int Alignment, typename ParamOp_ = nullptr_t>
      |                                                                                   ^~~~~~~~~
      |                                                                                   platform::nullptr_t
/export/users/zhaojiam/src/ByteTransformer/3rdparty/cutlass/include/cutlass/platform/platform.h:330:12: note: 'platform::nullptr_t' declared here
  330 | using std::nullptr_t;
      |            ^
error: unknown argument: '-Xcudafe'
error: unknown argument: '--diag_suppress=cc_clobber_ignored,--diag_suppress=field_without_dll_interface,--diag_suppress=base_class_has_different_dll_interface,--diag_suppress=dll_interface_conflict_none_assumed,--diag_suppress=dll_interface_conflict_dllexport_assumed,--diag_suppress=bad_friend_decl'
error: unknown argument: '-Xcudafe'
error: unknown argument: '--diag_suppress=cc_clobber_ignored,--diag_suppress=field_without_dll_interface,--diag_suppress=base_class_has_different_dll_interface,--diag_suppress=dll_interface_conflict_none_assumed,--diag_suppress=dll_interface_conflict_dllexport_assumed,--diag_suppress=bad_friend_decl'
error: unknown argument: '-Xcudafe'
error: unknown argument: '--diag_suppress=cc_clobber_ignored,--diag_suppress=field_without_dll_interface,--diag_suppress=base_class_has_different_dll_interface,--diag_suppress=dll_interface_conflict_none_assumed,--diag_suppress=dll_interface_conflict_dllexport_assumed,--diag_suppress=bad_friend_decl'
In file included from /export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/gemm_bias_act.cu:20:
In file included from /export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/gemm_bias_act.h:24:
In file included from /export/users/zhaojiam/src/ByteTransformer/3rdparty/cutlass/include/cutlass/gemm/device/gemm.h:45:
In file included from /export/users/zhaojiam/src/ByteTransformer/3rdparty/cutlass/include/cutlass/gemm/kernel/default_gemm.h:50:
In file included from /export/users/zhaojiam/src/ByteTransformer/3rdparty/cutlass/include/cutlass/epilogue/threadblock/epilogue.h:62:
In file included from /export/users/zhaojiam/src/ByteTransformer/3rdparty/cutlass/include/cutlass/epilogue/threadblock/predicated_tile_iterator.h:51:
/export/users/zhaojiam/src/ByteTransformer/3rdparty/cutlass/include/cutlass/arch/memory.h:364:6: error: no function template matches function template specialization 'shared_load'
  364 | void shared_load<2>(void *dst, uint32_t ptr) {
      |      ^
/export/users/zhaojiam/src/ByteTransformer/3rdparty/cutlass/include/cutlass/arch/memory.h:359:6: note: candidate template ignored: target attributes do not match
  359 | void shared_load(void *dst, uint32_t ptr);
      |      ^
/export/users/zhaojiam/src/ByteTransformer/3rdparty/cutlass/include/cutlass/arch/memory.h:373:6: error: no function template matches function template specialization 'shared_load'
  373 | void shared_load<4>(void *dst, uint32_t ptr) {
      |      ^
/export/users/zhaojiam/src/ByteTransformer/3rdparty/cutlass/include/cutlass/arch/memory.h:359:6: note: candidate template ignored: target attributes do not match
  359 | void shared_load(void *dst, uint32_t ptr);
      |      ^
/export/users/zhaojiam/src/ByteTransformer/3rdparty/cutlass/include/cutlass/arch/memory.h:382:6: error: no function template matches function template specialization 'shared_load'
  382 | void shared_load<8>(void *dst, uint32_t ptr) {
      |      ^
/export/users/zhaojiam/src/ByteTransformer/3rdparty/cutlass/include/cutlass/arch/memory.h:359:6: note: candidate template ignored: target attributes do not match
  359 | void shared_load(void *dst, uint32_t ptr);
      |      ^
/export/users/zhaojiam/src/ByteTransformer/3rdparty/cutlass/include/cutlass/arch/memory.h:394:6: error: no function template matches function template specialization 'shared_load'
  394 | void shared_load<16>(void *dst, uint32_t ptr) {
      |      ^
/export/users/zhaojiam/src/ByteTransformer/3rdparty/cutlass/include/cutlass/arch/memory.h:359:6: note: candidate template ignored: target attributes do not match
  359 | void shared_load(void *dst, uint32_t ptr);
      |      ^
/export/users/zhaojiam/src/ByteTransformer/3rdparty/cutlass/include/cutlass/arch/memory.h:415:6: error: no function template matches function template specialization 'shared_store'
  415 | void shared_store<2>(uint32_t ptr, void const *src) {
      |      ^
/export/users/zhaojiam/src/ByteTransformer/3rdparty/cutlass/include/cutlass/arch/memory.h:410:6: note: candidate template ignored: target attributes do not match
  410 | void shared_store(uint32_t ptr, void const *src);
      |      ^
/export/users/zhaojiam/src/ByteTransformer/3rdparty/cutlass/include/cutlass/arch/memory.h:426:6: error: no function template matches function template specialization 'shared_store'
  426 | void shared_store<4>(uint32_t ptr, void const *src) {
      |      ^
/export/users/zhaojiam/src/ByteTransformer/3rdparty/cutlass/include/cutlass/arch/memory.h:410:6: note: candidate template ignored: target attributes do not match
  410 | void shared_store(uint32_t ptr, void const *src);
      |      ^
/export/users/zhaojiam/src/ByteTransformer/3rdparty/cutlass/include/cutlass/arch/memory.h:437:6: error: no function template matches function template specialization 'shared_store'
  437 | void shared_store<8>(uint32_t ptr, void const *src) {
      |      ^
/export/users/zhaojiam/src/ByteTransformer/3rdparty/cutlass/include/cutlass/arch/memory.h:410:6: note: candidate template ignored: target attributes do not match
  410 | void shared_store(uint32_t ptr, void const *src);
      |      ^
/export/users/zhaojiam/src/ByteTransformer/3rdparty/cutlass/include/cutlass/arch/memory.h:450:6: error: no function template matches function template specialization 'shared_store'
  450 | void shared_store<16>(uint32_t ptr, void const *src) {
      |      ^
/export/users/zhaojiam/src/ByteTransformer/3rdparty/cutlass/include/cutlass/arch/memory.h:410:6: note: candidate template ignored: target attributes do not match
  410 | void shared_store(uint32_t ptr, void const *src);
      |      ^
error: unknown argument: '-Xcudafe'
error: unknown argument: '--diag_suppress=cc_clobber_ignored,--diag_suppress=field_without_dll_interface,--diag_suppress=base_class_has_different_dll_interface,--diag_suppress=dll_interface_conflict_none_assumed,--diag_suppress=dll_interface_conflict_dllexport_assumed,--diag_suppress=bad_friend_decl'
error: unknown argument: '-Xcudafe'
error: unknown argument: '--diag_suppress=cc_clobber_ignored,--diag_suppress=field_without_dll_interface,--diag_suppress=base_class_has_different_dll_interface,--diag_suppress=dll_interface_conflict_none_assumed,--diag_suppress=dll_interface_conflict_dllexport_assumed,--diag_suppress=bad_friend_decl'
In file included from /export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/cutlass_attention.cu:23:
In file included from /export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/cutlass_attention.h:25:
/export/users/zhaojiam/src/ByteTransformer/cutlass_contrib/include/cutlass/contrib/args_pack_def.h:28:83: error: unknown type name 'nullptr_t'; did you mean 'platform::nullptr_t'?
   28 | template <typename Element_, typename Layout_, int Alignment, typename ParamOp_ = nullptr_t>
      |                                                                                   ^~~~~~~~~
      |                                                                                   platform::nullptr_t
/export/users/zhaojiam/src/ByteTransformer/3rdparty/cutlass/include/cutlass/platform/platform.h:330:12: note: 'platform::nullptr_t' declared here
  330 | using std::nullptr_t;
      |            ^
In file included from /export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/cutlass_attention.cu:24:
In file included from /export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/cutlass_attention_defs.h:25:
In file included from /export/users/zhaojiam/src/ByteTransformer/cutlass_contrib/include/cutlass/contrib/epilogue/threadblock/softmax_epilogue.h:56:
In file included from /export/users/zhaojiam/src/ByteTransformer/3rdparty/cutlass/include/cutlass/epilogue/threadblock/predicated_tile_iterator.h:51:
/export/users/zhaojiam/src/ByteTransformer/3rdparty/cutlass/include/cutlass/arch/memory.h:364:6: error: no function template matches function template specialization 'shared_load'
  364 | void shared_load<2>(void *dst, uint32_t ptr) {
      |      ^
/export/users/zhaojiam/src/ByteTransformer/3rdparty/cutlass/include/cutlass/arch/memory.h:359:6: note: candidate template ignored: target attributes do not match
  359 | void shared_load(void *dst, uint32_t ptr);
      |      ^
/export/users/zhaojiam/src/ByteTransformer/3rdparty/cutlass/include/cutlass/arch/memory.h:373:6: error: no function template matches function template specialization 'shared_load'
  373 | void shared_load<4>(void *dst, uint32_t ptr) {
      |      ^
/export/users/zhaojiam/src/ByteTransformer/3rdparty/cutlass/include/cutlass/arch/memory.h:359:6: note: candidate template ignored: target attributes do not match
  359 | void shared_load(void *dst, uint32_t ptr);
      |      ^
/export/users/zhaojiam/src/ByteTransformer/3rdparty/cutlass/include/cutlass/arch/memory.h:382:6: error: no function template matches function template specialization 'shared_load'
  382 | void shared_load<8>(void *dst, uint32_t ptr) {
      |      ^
/export/users/zhaojiam/src/ByteTransformer/3rdparty/cutlass/include/cutlass/arch/memory.h:359:6: note: candidate template ignored: target attributes do not match
  359 | void shared_load(void *dst, uint32_t ptr);
      |      ^
/export/users/zhaojiam/src/ByteTransformer/3rdparty/cutlass/include/cutlass/arch/memory.h:394:6: error: no function template matches function template specialization 'shared_load'
  394 | void shared_load<16>(void *dst, uint32_t ptr) {
      |      ^
/export/users/zhaojiam/src/ByteTransformer/3rdparty/cutlass/include/cutlass/arch/memory.h:359:6: note: candidate template ignored: target attributes do not match
  359 | void shared_load(void *dst, uint32_t ptr);
      |      ^
/export/users/zhaojiam/src/ByteTransformer/3rdparty/cutlass/include/cutlass/arch/memory.h:415:6: error: no function template matches function template specialization 'shared_store'
  415 | void shared_store<2>(uint32_t ptr, void const *src) {
      |      ^
/export/users/zhaojiam/src/ByteTransformer/3rdparty/cutlass/include/cutlass/arch/memory.h:410:6: note: candidate template ignored: target attributes do not match
  410 | void shared_store(uint32_t ptr, void const *src);
      |      ^
/export/users/zhaojiam/src/ByteTransformer/3rdparty/cutlass/include/cutlass/arch/memory.h:426:6: error: no function template matches function template specialization 'shared_store'
  426 | void shared_store<4>(uint32_t ptr, void const *src) {
      |      ^
/export/users/zhaojiam/src/ByteTransformer/3rdparty/cutlass/include/cutlass/arch/memory.h:410:6: note: candidate template ignored: target attributes do not match
  410 | void shared_store(uint32_t ptr, void const *src);
      |      ^
/export/users/zhaojiam/src/ByteTransformer/3rdparty/cutlass/include/cutlass/arch/memory.h:437:6: error: no function template matches function template specialization 'shared_store'
  437 | void shared_store<8>(uint32_t ptr, void const *src) {
      |      ^
/export/users/zhaojiam/src/ByteTransformer/3rdparty/cutlass/include/cutlass/arch/memory.h:410:6: note: candidate template ignored: target attributes do not match
  410 | void shared_store(uint32_t ptr, void const *src);
      |      ^
/export/users/zhaojiam/src/ByteTransformer/3rdparty/cutlass/include/cutlass/arch/memory.h:450:6: error: no function template matches function template specialization 'shared_store'
  450 | void shared_store<16>(uint32_t ptr, void const *src) {
      |      ^
/export/users/zhaojiam/src/ByteTransformer/3rdparty/cutlass/include/cutlass/arch/memory.h:410:6: note: candidate template ignored: target attributes do not match
  410 | void shared_store(uint32_t ptr, void const *src);
      |      ^
In file included from /export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/cutlass_attention.cu:24:
In file included from /export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/cutlass_attention_defs.h:27:
In file included from /export/users/zhaojiam/src/ByteTransformer/cutlass_contrib/include/cutlass/contrib/gemm/kernel/default_gemm_grouped.h:59:
In file included from /export/users/zhaojiam/src/ByteTransformer/cutlass_contrib/include/cutlass/contrib/gemm/threadblock/default_mma.h:50:
/export/users/zhaojiam/src/ByteTransformer/cutlass_contrib/include/cutlass/contrib/gemm/threadblock/mma_multistage.h:309:27: error: use 'template' keyword to treat 'Detail' as a dependent template name
  309 |     typename PrologueOpA::Detail<WarpLoadIteratorA>::Fragment prologue_frag_A;
      |                           ^
      |                           template 
/export/users/zhaojiam/src/ByteTransformer/cutlass_contrib/include/cutlass/contrib/gemm/threadblock/mma_multistage.h:310:27: error: use 'template' keyword to treat 'Detail' as a dependent template name
  310 |     typename PrologueOpB::Detail<WarpLoadIteratorB>::Fragment prologue_frag_B;
      |                           ^
      |                           template 
error: unknown argument: '-Xcudafe'
error: unknown argument: '--diag_suppress=cc_clobber_ignored,--diag_suppress=field_without_dll_interface,--diag_suppress=base_class_has_different_dll_interface,--diag_suppress=dll_interface_conflict_none_assumed,--diag_suppress=dll_interface_conflict_dllexport_assumed,--diag_suppress=bad_friend_decl'
error: unknown argument: '-Xcudafe'
error: unknown argument: '--diag_suppress=cc_clobber_ignored,--diag_suppress=field_without_dll_interface,--diag_suppress=base_class_has_different_dll_interface,--diag_suppress=dll_interface_conflict_none_assumed,--diag_suppress=dll_interface_conflict_dllexport_assumed,--diag_suppress=bad_friend_decl'
error: unknown argument: '-Xcudafe'
error: unknown argument: '--diag_suppress=cc_clobber_ignored,--diag_suppress=field_without_dll_interface,--diag_suppress=base_class_has_different_dll_interface,--diag_suppress=dll_interface_conflict_none_assumed,--diag_suppress=dll_interface_conflict_dllexport_assumed,--diag_suppress=bad_friend_decl'
In file included from /export/users/zhaojiam/src/ByteTransformer/unit_test/bert_transformer_test.cc:20:
In file included from /export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/bert_transformer.h:25:
In file included from /export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/cutlass_attention.h:25:
/export/users/zhaojiam/src/ByteTransformer/cutlass_contrib/include/cutlass/contrib/args_pack_def.h:28:83: error: unknown type name 'nullptr_t'; did you mean 'platform::nullptr_t'?
   28 | template <typename Element_, typename Layout_, int Alignment, typename ParamOp_ = nullptr_t>
      |                                                                                   ^~~~~~~~~
      |                                                                                   platform::nullptr_t
/export/users/zhaojiam/src/ByteTransformer/3rdparty/cutlass/include/cutlass/platform/platform.h:330:12: note: 'platform::nullptr_t' declared here
  330 | using std::nullptr_t;
      |            ^
In file included from /export/users/zhaojiam/src/ByteTransformer/unit_test/bert_transformer_test.cc:21:
/export/users/zhaojiam/src/ByteTransformer/unit_test/helper.h:36:28: error: invalid application of 'sizeof' to an incomplete type 'void'
   36 |   cudaMalloc((void **)ptr, sizeof(T) * size);
      |                            ^~~~~~~~~
/export/users/zhaojiam/src/ByteTransformer/unit_test/bert_transformer_test.cc:162:3: note: in instantiation of function template specialization 'device_malloc<void>' requested here
  162 |   device_malloc((void **)&buf, buf_size);
      |   ^
error: unknown argument: '-Xcudafe'
error: unknown argument: '--diag_suppress=cc_clobber_ignored,--diag_suppress=field_without_dll_interface,--diag_suppress=base_class_has_different_dll_interface,--diag_suppress=dll_interface_conflict_none_assumed,--diag_suppress=dll_interface_conflict_dllexport_assumed,--diag_suppress=bad_friend_decl'
error: unknown argument: '-Xcudafe'
error: unknown argument: '--diag_suppress=cc_clobber_ignored,--diag_suppress=field_without_dll_interface,--diag_suppress=base_class_has_different_dll_interface,--diag_suppress=dll_interface_conflict_none_assumed,--diag_suppress=dll_interface_conflict_dllexport_assumed,--diag_suppress=bad_friend_decl'
In file included from /export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/bert_transformer.cu:20:
In file included from /export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/bert_transformer.h:25:
In file included from /export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/cutlass_attention.h:25:
/export/users/zhaojiam/src/ByteTransformer/cutlass_contrib/include/cutlass/contrib/args_pack_def.h:28:83: error: unknown type name 'nullptr_t'; did you mean 'platform::nullptr_t'?
   28 | template <typename Element_, typename Layout_, int Alignment, typename ParamOp_ = nullptr_t>
      |                                                                                   ^~~~~~~~~
      |                                                                                   platform::nullptr_t
/export/users/zhaojiam/src/ByteTransformer/3rdparty/cutlass/include/cutlass/platform/platform.h:330:12: note: 'platform::nullptr_t' declared here
  330 | using std::nullptr_t;
      |            ^
In file included from /export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/bert_transformer.cu:22:
In file included from /export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/gemm_bias_act.h:24:
In file included from /export/users/zhaojiam/src/ByteTransformer/3rdparty/cutlass/include/cutlass/gemm/device/gemm.h:45:
In file included from /export/users/zhaojiam/src/ByteTransformer/3rdparty/cutlass/include/cutlass/gemm/kernel/default_gemm.h:50:
In file included from /export/users/zhaojiam/src/ByteTransformer/3rdparty/cutlass/include/cutlass/epilogue/threadblock/epilogue.h:62:
In file included from /export/users/zhaojiam/src/ByteTransformer/3rdparty/cutlass/include/cutlass/epilogue/threadblock/predicated_tile_iterator.h:51:
/export/users/zhaojiam/src/ByteTransformer/3rdparty/cutlass/include/cutlass/arch/memory.h:364:6: error: no function template matches function template specialization 'shared_load'
  364 | void shared_load<2>(void *dst, uint32_t ptr) {
      |      ^
/export/users/zhaojiam/src/ByteTransformer/3rdparty/cutlass/include/cutlass/arch/memory.h:359:6: note: candidate template ignored: target attributes do not match
  359 | void shared_load(void *dst, uint32_t ptr);
      |      ^
/export/users/zhaojiam/src/ByteTransformer/3rdparty/cutlass/include/cutlass/arch/memory.h:373:6: error: no function template matches function template specialization 'shared_load'
  373 | void shared_load<4>(void *dst, uint32_t ptr) {
      |      ^
/export/users/zhaojiam/src/ByteTransformer/3rdparty/cutlass/include/cutlass/arch/memory.h:359:6: note: candidate template ignored: target attributes do not match
  359 | void shared_load(void *dst, uint32_t ptr);
      |      ^
/export/users/zhaojiam/src/ByteTransformer/3rdparty/cutlass/include/cutlass/arch/memory.h:382:6: error: no function template matches function template specialization 'shared_load'
  382 | void shared_load<8>(void *dst, uint32_t ptr) {
      |      ^
/export/users/zhaojiam/src/ByteTransformer/3rdparty/cutlass/include/cutlass/arch/memory.h:359:6: note: candidate template ignored: target attributes do not match
  359 | void shared_load(void *dst, uint32_t ptr);
      |      ^
/export/users/zhaojiam/src/ByteTransformer/3rdparty/cutlass/include/cutlass/arch/memory.h:394:6: error: no function template matches function template specialization 'shared_load'
  394 | void shared_load<16>(void *dst, uint32_t ptr) {
      |      ^
/export/users/zhaojiam/src/ByteTransformer/3rdparty/cutlass/include/cutlass/arch/memory.h:359:6: note: candidate template ignored: target attributes do not match
  359 | void shared_load(void *dst, uint32_t ptr);
      |      ^
/export/users/zhaojiam/src/ByteTransformer/3rdparty/cutlass/include/cutlass/arch/memory.h:415:6: error: no function template matches function template specialization 'shared_store'
  415 | void shared_store<2>(uint32_t ptr, void const *src) {
      |      ^
/export/users/zhaojiam/src/ByteTransformer/3rdparty/cutlass/include/cutlass/arch/memory.h:410:6: note: candidate template ignored: target attributes do not match
  410 | void shared_store(uint32_t ptr, void const *src);
      |      ^
/export/users/zhaojiam/src/ByteTransformer/3rdparty/cutlass/include/cutlass/arch/memory.h:426:6: error: no function template matches function template specialization 'shared_store'
  426 | void shared_store<4>(uint32_t ptr, void const *src) {
      |      ^
/export/users/zhaojiam/src/ByteTransformer/3rdparty/cutlass/include/cutlass/arch/memory.h:410:6: note: candidate template ignored: target attributes do not match
  410 | void shared_store(uint32_t ptr, void const *src);
      |      ^
/export/users/zhaojiam/src/ByteTransformer/3rdparty/cutlass/include/cutlass/arch/memory.h:437:6: error: no function template matches function template specialization 'shared_store'
  437 | void shared_store<8>(uint32_t ptr, void const *src) {
      |      ^
/export/users/zhaojiam/src/ByteTransformer/3rdparty/cutlass/include/cutlass/arch/memory.h:410:6: note: candidate template ignored: target attributes do not match
  410 | void shared_store(uint32_t ptr, void const *src);
      |      ^
/export/users/zhaojiam/src/ByteTransformer/3rdparty/cutlass/include/cutlass/arch/memory.h:450:6: error: no function template matches function template specialization 'shared_store'
  450 | void shared_store<16>(uint32_t ptr, void const *src) {
      |      ^
/export/users/zhaojiam/src/ByteTransformer/3rdparty/cutlass/include/cutlass/arch/memory.h:410:6: note: candidate template ignored: target attributes do not match
  410 | void shared_store(uint32_t ptr, void const *src);
      |      ^
In file included from /export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused_long.cu:21:
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/reduce.h:28:3: warning: DPCT1053:0: Migration of device assembly code is not supported.
   28 |   asm("mov.s32 %0, %laneid;" : "=r"(laneId));
      |   ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused_long.cu:31:1: warning: DPCT1110:1: The total declared local variable size in device function wmma_attention_long_kernel exceeds 128 bytes and may cause high register pressure. Consult with your hardware vendor to find the total register size available and adjust the code, or use smaller sub-group size to avoid high register pressure.
   31 | __global__ void wmma_attention_long_kernel(const half2 *qkv, const half2 *qkv_bias,
      | ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused_long.cu:74:11: warning: DPCT1082:2: Migration of nvcuda::wmma::fragment<wmma::matrix_a, 16, 16, 16, half, wmma::row_major> type is not supported.
   74 |     wmma::fragment<wmma::matrix_a, WMMA_M, WMMA_N, WMMA_K, half, wmma::row_major> Q_mat;
      |           ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused_long.cu:74:26: warning: DPCT1082:3: Migration of nvcuda::wmma::matrix_a type is not supported.
   74 |     wmma::fragment<wmma::matrix_a, WMMA_M, WMMA_N, WMMA_K, half, wmma::row_major> Q_mat;
      |                          ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused_long.cu:74:72: warning: DPCT1082:4: Migration of nvcuda::wmma::row_major type is not supported.
   74 |     wmma::fragment<wmma::matrix_a, WMMA_M, WMMA_N, WMMA_K, half, wmma::row_major> Q_mat;
      |                                                                        ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused_long.cu:75:11: warning: DPCT1082:5: Migration of nvcuda::wmma::fragment<wmma::matrix_b, 16, 16, 16, half, wmma::col_major> type is not supported.
   75 |     wmma::fragment<wmma::matrix_b, WMMA_M, WMMA_N, WMMA_K, half, wmma::col_major> K_mat;
      |           ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused_long.cu:75:26: warning: DPCT1082:6: Migration of nvcuda::wmma::matrix_b type is not supported.
   75 |     wmma::fragment<wmma::matrix_b, WMMA_M, WMMA_N, WMMA_K, half, wmma::col_major> K_mat;
      |                          ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused_long.cu:75:72: warning: DPCT1082:7: Migration of nvcuda::wmma::col_major type is not supported.
   75 |     wmma::fragment<wmma::matrix_b, WMMA_M, WMMA_N, WMMA_K, half, wmma::col_major> K_mat;
      |                                                                        ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused_long.cu:76:11: warning: DPCT1082:8: Migration of nvcuda::wmma::fragment<wmma::accumulator, 16, 16, 16, half> type is not supported.
   76 |     wmma::fragment<wmma::accumulator, WMMA_M, WMMA_N, WMMA_K, half> QK_mat;
      |           ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused_long.cu:76:26: warning: DPCT1082:9: Migration of nvcuda::wmma::accumulator type is not supported.
   76 |     wmma::fragment<wmma::accumulator, WMMA_M, WMMA_N, WMMA_K, half> QK_mat;
      |                          ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused_long.cu:77:5: warning: DPCT1007:10: Migration of nvcuda::wmma::fill_fragment is not supported.
   77 |     wmma::fill_fragment(QK_mat, 0.0f);
      |     ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused_long.cu:86:7: warning: DPCT1007:11: Migration of nvcuda::wmma::mma_sync is not supported.
   86 |       wmma::mma_sync(QK_mat, Q_mat, K_mat, QK_mat);
      |       ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused_long.cu:145:11: warning: DPCT1082:12: Migration of nvcuda::wmma::fragment<wmma::matrix_a, 16, 16, 16, half, wmma::row_major> type is not supported.
  145 |     wmma::fragment<wmma::matrix_a, WMMA_M, WMMA_N, WMMA_K, half, wmma::row_major> Logits_mat;
      |           ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused_long.cu:145:26: warning: DPCT1082:13: Migration of nvcuda::wmma::matrix_a type is not supported.
  145 |     wmma::fragment<wmma::matrix_a, WMMA_M, WMMA_N, WMMA_K, half, wmma::row_major> Logits_mat;
      |                          ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused_long.cu:145:72: warning: DPCT1082:14: Migration of nvcuda::wmma::row_major type is not supported.
  145 |     wmma::fragment<wmma::matrix_a, WMMA_M, WMMA_N, WMMA_K, half, wmma::row_major> Logits_mat;
      |                                                                        ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused_long.cu:146:11: warning: DPCT1082:15: Migration of nvcuda::wmma::fragment<wmma::matrix_b, 16, 16, 16, half, wmma::row_major> type is not supported.
  146 |     wmma::fragment<wmma::matrix_b, WMMA_M, WMMA_N, WMMA_K, half, wmma::row_major> V_mat;
      |           ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused_long.cu:146:26: warning: DPCT1082:16: Migration of nvcuda::wmma::matrix_b type is not supported.
  146 |     wmma::fragment<wmma::matrix_b, WMMA_M, WMMA_N, WMMA_K, half, wmma::row_major> V_mat;
      |                          ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused_long.cu:146:72: warning: DPCT1082:17: Migration of nvcuda::wmma::row_major type is not supported.
  146 |     wmma::fragment<wmma::matrix_b, WMMA_M, WMMA_N, WMMA_K, half, wmma::row_major> V_mat;
      |                                                                        ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused_long.cu:147:11: warning: DPCT1082:18: Migration of nvcuda::wmma::fragment<wmma::accumulator, 16, 16, 16, half> type is not supported.
  147 |     wmma::fragment<wmma::accumulator, WMMA_M, WMMA_N, WMMA_K, half> QKV_mat;
      |           ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused_long.cu:147:26: warning: DPCT1082:19: Migration of nvcuda::wmma::accumulator type is not supported.
  147 |     wmma::fragment<wmma::accumulator, WMMA_M, WMMA_N, WMMA_K, half> QKV_mat;
      |                          ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused_long.cu:148:5: warning: DPCT1007:20: Migration of nvcuda::wmma::fill_fragment is not supported.
  148 |     wmma::fill_fragment(QKV_mat, 0.0f);
      |     ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused_long.cu:157:7: warning: DPCT1007:21: Migration of nvcuda::wmma::mma_sync is not supported.
  157 |       wmma::mma_sync(QKV_mat, Logits_mat, V_mat, QKV_mat);
      |       ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused_long.cu:88:5: warning: DPCT1007:22: Migration of nvcuda::wmma::store_matrix_sync is not supported.
   88 |     wmma::store_matrix_sync(s_logits[warp_from_offset] + warp_to_offset, QK_mat,
      |     ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused_long.cu:83:7: warning: DPCT1007:23: Migration of nvcuda::wmma::load_matrix_sync is not supported.
   83 |       wmma::load_matrix_sync(Q_mat, s_query[warp_from_offset] + k * WMMA_K,
      |       ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused_long.cu:85:7: warning: DPCT1007:24: Migration of nvcuda::wmma::load_matrix_sync is not supported.
   85 |       wmma::load_matrix_sync(K_mat, s_kv[warp_to_offset] + k * WMMA_K, size_per_head + SKEW_HALF);
      |       ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused_long.cu:159:5: warning: DPCT1007:25: Migration of nvcuda::wmma::store_matrix_sync is not supported.
  159 |     wmma::store_matrix_sync(s_query[warp_from_offset] + warp_to_offset, QKV_mat,
      |     ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused_long.cu:154:7: warning: DPCT1007:26: Migration of nvcuda::wmma::load_matrix_sync is not supported.
  154 |       wmma::load_matrix_sync(Logits_mat, s_logits[warp_from_offset] + k * WMMA_K,
      |       ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused_long.cu:156:7: warning: DPCT1007:27: Migration of nvcuda::wmma::load_matrix_sync is not supported.
  156 |       wmma::load_matrix_sync(V_mat, s_kv[k * WMMA_K] + warp_to_offset, size_per_head + SKEW_HALF);
      |       ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused_long.cu:172:1: warning: DPCT1110:28: The total declared local variable size in device function wmma_attention_long_rm_kernel exceeds 128 bytes and may cause high register pressure. Consult with your hardware vendor to find the total register size available and adjust the code, or use smaller sub-group size to avoid high register pressure.
  172 | __global__ void wmma_attention_long_rm_kernel(const half2 *qkv, const half2 *qkv_bias,
      | ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused_long.cu:221:11: warning: DPCT1082:29: Migration of nvcuda::wmma::fragment<wmma::matrix_a, 16, 16, 16, half, wmma::row_major> type is not supported.
  221 |     wmma::fragment<wmma::matrix_a, WMMA_M, WMMA_N, WMMA_K, half, wmma::row_major> Q_mat;
      |           ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused_long.cu:221:26: warning: DPCT1082:30: Migration of nvcuda::wmma::matrix_a type is not supported.
  221 |     wmma::fragment<wmma::matrix_a, WMMA_M, WMMA_N, WMMA_K, half, wmma::row_major> Q_mat;
      |                          ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused_long.cu:221:72: warning: DPCT1082:31: Migration of nvcuda::wmma::row_major type is not supported.
  221 |     wmma::fragment<wmma::matrix_a, WMMA_M, WMMA_N, WMMA_K, half, wmma::row_major> Q_mat;
      |                                                                        ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused_long.cu:222:11: warning: DPCT1082:32: Migration of nvcuda::wmma::fragment<wmma::matrix_b, 16, 16, 16, half, wmma::col_major> type is not supported.
  222 |     wmma::fragment<wmma::matrix_b, WMMA_M, WMMA_N, WMMA_K, half, wmma::col_major> K_mat;
      |           ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused_long.cu:222:26: warning: DPCT1082:33: Migration of nvcuda::wmma::matrix_b type is not supported.
  222 |     wmma::fragment<wmma::matrix_b, WMMA_M, WMMA_N, WMMA_K, half, wmma::col_major> K_mat;
      |                          ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused_long.cu:222:72: warning: DPCT1082:34: Migration of nvcuda::wmma::col_major type is not supported.
  222 |     wmma::fragment<wmma::matrix_b, WMMA_M, WMMA_N, WMMA_K, half, wmma::col_major> K_mat;
      |                                                                        ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused_long.cu:223:11: warning: DPCT1082:35: Migration of nvcuda::wmma::fragment<wmma::accumulator, 16, 16, 16, half> type is not supported.
  223 |     wmma::fragment<wmma::accumulator, WMMA_M, WMMA_N, WMMA_K, half> QK_mat;
      |           ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused_long.cu:223:26: warning: DPCT1082:36: Migration of nvcuda::wmma::accumulator type is not supported.
  223 |     wmma::fragment<wmma::accumulator, WMMA_M, WMMA_N, WMMA_K, half> QK_mat;
      |                          ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused_long.cu:224:5: warning: DPCT1007:37: Migration of nvcuda::wmma::fill_fragment is not supported.
  224 |     wmma::fill_fragment(QK_mat, 0.0f);
      |     ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused_long.cu:235:9: warning: DPCT1007:38: Migration of nvcuda::wmma::mma_sync is not supported.
  235 |         wmma::mma_sync(QK_mat, Q_mat, K_mat, QK_mat);
      |         ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused_long.cu:288:11: warning: DPCT1082:39: Migration of nvcuda::wmma::fragment<wmma::matrix_a, 16, 16, 16, half, wmma::row_major> type is not supported.
  288 |     wmma::fragment<wmma::matrix_a, WMMA_M, WMMA_N, WMMA_K, half, wmma::row_major> Logits_mat;
      |           ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused_long.cu:288:26: warning: DPCT1082:40: Migration of nvcuda::wmma::matrix_a type is not supported.
  288 |     wmma::fragment<wmma::matrix_a, WMMA_M, WMMA_N, WMMA_K, half, wmma::row_major> Logits_mat;
      |                          ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused_long.cu:288:72: warning: DPCT1082:41: Migration of nvcuda::wmma::row_major type is not supported.
  288 |     wmma::fragment<wmma::matrix_a, WMMA_M, WMMA_N, WMMA_K, half, wmma::row_major> Logits_mat;
      |                                                                        ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused_long.cu:289:11: warning: DPCT1082:42: Migration of nvcuda::wmma::fragment<wmma::matrix_b, 16, 16, 16, half, wmma::row_major> type is not supported.
  289 |     wmma::fragment<wmma::matrix_b, WMMA_M, WMMA_N, WMMA_K, half, wmma::row_major> V_mat;
      |           ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused_long.cu:289:26: warning: DPCT1082:43: Migration of nvcuda::wmma::matrix_b type is not supported.
  289 |     wmma::fragment<wmma::matrix_b, WMMA_M, WMMA_N, WMMA_K, half, wmma::row_major> V_mat;
      |                          ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused_long.cu:289:72: warning: DPCT1082:44: Migration of nvcuda::wmma::row_major type is not supported.
  289 |     wmma::fragment<wmma::matrix_b, WMMA_M, WMMA_N, WMMA_K, half, wmma::row_major> V_mat;
      |                                                                        ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused_long.cu:290:11: warning: DPCT1082:45: Migration of nvcuda::wmma::fragment<wmma::accumulator, 16, 16, 16, half> type is not supported.
  290 |     wmma::fragment<wmma::accumulator, WMMA_M, WMMA_N, WMMA_K, half> QKV_mat;
      |           ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused_long.cu:290:26: warning: DPCT1082:46: Migration of nvcuda::wmma::accumulator type is not supported.
  290 |     wmma::fragment<wmma::accumulator, WMMA_M, WMMA_N, WMMA_K, half> QKV_mat;
      |                          ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused_long.cu:291:5: warning: DPCT1007:47: Migration of nvcuda::wmma::fill_fragment is not supported.
  291 |     wmma::fill_fragment(QKV_mat, 0.0f);
      |     ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused_long.cu:300:7: warning: DPCT1007:48: Migration of nvcuda::wmma::mma_sync is not supported.
  300 |       wmma::mma_sync(QKV_mat, Logits_mat, V_mat, QKV_mat);
      |       ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused_long.cu:237:7: warning: DPCT1007:49: Migration of nvcuda::wmma::store_matrix_sync is not supported.
  237 |       wmma::store_matrix_sync(s_logits[warp_from_offset] + warp_to_offset, QK_mat,
      |       ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused_long.cu:231:9: warning: DPCT1007:50: Migration of nvcuda::wmma::load_matrix_sync is not supported.
  231 |         wmma::load_matrix_sync(Q_mat, s_query[warp_from_offset] + k * WMMA_K,
      |         ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused_long.cu:233:9: warning: DPCT1007:51: Migration of nvcuda::wmma::load_matrix_sync is not supported.
  233 |         wmma::load_matrix_sync(K_mat, s_kv[warp_to_offset] + k * WMMA_K,
      |         ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused_long.cu:302:5: warning: DPCT1007:52: Migration of nvcuda::wmma::store_matrix_sync is not supported.
  302 |     wmma::store_matrix_sync(s_query[warp_from_offset] + warp_to_offset, QKV_mat,
      |     ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused_long.cu:297:7: warning: DPCT1007:53: Migration of nvcuda::wmma::load_matrix_sync is not supported.
  297 |       wmma::load_matrix_sync(Logits_mat, s_logits[warp_from_offset] + k * WMMA_K,
      |       ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused_long.cu:299:7: warning: DPCT1007:54: Migration of nvcuda::wmma::load_matrix_sync is not supported.
  299 |       wmma::load_matrix_sync(V_mat, s_kv[k * WMMA_K] + warp_to_offset, size_per_head + SKEW_HALF);
      |       ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused_long.cu:327:3: warning: DPCT1049:55: The work-group size passed to the SYCL kernel may exceed the limit. To get the device limit, query info::device::max_work_group_size. Adjust the work-group size if needed.
  327 |   wmma_attention_long_kernel<SEQ_LEN, SIZE_PER_HEAD, SPLIT_LEN>                                   \
      |   ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused_long.cu:341:3: warning: DPCT1049:56: The work-group size passed to the SYCL kernel may exceed the limit. To get the device limit, query info::device::max_work_group_size. Adjust the work-group size if needed.
  341 |   wmma_attention_long_rm_kernel<SEQ_LEN, SIZE_PER_HEAD, SPLIT_LEN>                                \
      |   ^
In file included from /export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/gemm_bias_act.cu:20:
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/gemm_bias_act.h:58:3: warning: DPCT1053:57: Migration of device assembly code is not supported.
   58 |   asm("rcp.approx.ftz.f32 %0,%1;\n\t" : "=f"(r) : "f"(d));
      |   ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/gemm_bias_act.cu:65:3: warning: DPCT1049:58: The work-group size passed to the SYCL kernel may exceed the limit. To get the device limit, query info::device::max_work_group_size. Adjust the work-group size if needed.
   65 |   add_bias_gelu<<<m, n / 8, 0, stream>>>(C, bias, m, n);
      |   ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/gemm_bias_act.cu:73:3: warning: DPCT1049:59: The work-group size passed to the SYCL kernel may exceed the limit. To get the device limit, query info::device::max_work_group_size. Adjust the work-group size if needed.
   73 |   add_bias_gelu<<<m_, n_ / 4, 0, stream>>>(C_, bias_, m_, n_);
      |   ^
In file included from /export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_nofused.cu:23:
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/softmax.h:192:1: warning: DPCT1110:60: The total declared local variable size in device function softmax_kernel_warp_half2_register exceeds 128 bytes and may cause high register pressure. Consult with your hardware vendor to find the total register size available and adjust the code, or use smaller sub-group size to avoid high register pressure.
  192 | __global__ void softmax_kernel_warp_half2_register(half2 *qk_buf, const half2 *atten_bias,
      | ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/softmax.h:245:1: warning: DPCT1110:61: The total declared local variable size in device function softmax_kernel_warp_half2_register_et exceeds 128 bytes and may cause high register pressure. Consult with your hardware vendor to find the total register size available and adjust the code, or use smaller sub-group size to avoid high register pressure.
  245 | __global__ void softmax_kernel_warp_half2_register_et(half2 *qk_buf, const half2 *atten_bias,
      | ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/softmax.h:301:5: warning: DPCT1049:62: The work-group size passed to the SYCL kernel may exceed the limit. To get the device limit, query info::device::max_work_group_size. Adjust the work-group size if needed.
  301 |     softmax_kernel_warp_half2_register<half2, REG_COUNT, false>                                 \
      |     ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/softmax.h:305:5: warning: DPCT1049:63: The work-group size passed to the SYCL kernel may exceed the limit. To get the device limit, query info::device::max_work_group_size. Adjust the work-group size if needed.
  305 |     softmax_kernel_warp_half2_register<half2, REG_COUNT, true><<<grid, block, 0, stream>>>(     \
      |     ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/softmax.h:384:7: warning: DPCT1049:64: The work-group size passed to the SYCL kernel may exceed the limit. To get the device limit, query info::device::max_work_group_size. Adjust the work-group size if needed.
  384 |       softmax_kernel_warp_half2<half2><<<grid, block, shmem_size, stream>>>(
      |       ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/softmax.h:324:26: warning: DPCT1083:65: The size of local memory in the migrated code may be different from the original code. Check that the allocated memory size in the migrated code is correct.
  324 |   const int shmem_size = head_num * seq_len * sizeof(float);
      |                          ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/softmax.h:392:5: warning: DPCT1049:66: The work-group size passed to the SYCL kernel may exceed the limit. To get the device limit, query info::device::max_work_group_size. Adjust the work-group size if needed.
  392 |     softmax_kernel_warp<T><<<grid, block, shmem_size, stream>>>(qk_buf, atten_bias, atten_mask,
      |     ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/softmax.h:310:5: warning: DPCT1049:67: The work-group size passed to the SYCL kernel may exceed the limit. To get the device limit, query info::device::max_work_group_size. Adjust the work-group size if needed.
  310 |     softmax_kernel_warp_half2_register_et<half2, REG_COUNT, false>                              \
      |     ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/softmax.h:314:5: warning: DPCT1049:68: The work-group size passed to the SYCL kernel may exceed the limit. To get the device limit, query info::device::max_work_group_size. Adjust the work-group size if needed.
  314 |     softmax_kernel_warp_half2_register_et<half2, REG_COUNT, true>                               \
      |     ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/softmax.h:464:7: warning: DPCT1049:69: The work-group size passed to the SYCL kernel may exceed the limit. To get the device limit, query info::device::max_work_group_size. Adjust the work-group size if needed.
  464 |       softmax_kernel_warp_half2_et<half2><<<grid, block, shmem_size, stream>>>(
      |       ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/softmax.h:404:26: warning: DPCT1083:70: The size of local memory in the migrated code may be different from the original code. Check that the allocated memory size in the migrated code is correct.
  404 |   const int shmem_size = head_num * seq_len * sizeof(float);
      |                          ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/softmax.h:472:5: warning: DPCT1049:71: The work-group size passed to the SYCL kernel may exceed the limit. To get the device limit, query info::device::max_work_group_size. Adjust the work-group size if needed.
  472 |     softmax_kernel_warp_et<T><<<grid, block, shmem_size, stream>>>(
      |     ^
In file included from /export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_nofused.cu:24:
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:31:1: warning: DPCT1110:72: The total declared local variable size in device function variety_wmma_attention_kernel exceeds 128 bytes and may cause high register pressure. Consult with your hardware vendor to find the total register size available and adjust the code, or use smaller sub-group size to avoid high register pressure.
   31 | __global__
      | ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:65:11: warning: DPCT1082:73: Migration of nvcuda::wmma::fragment<wmma::matrix_a, 16, 16, 16, half, wmma::row_major> type is not supported.
   65 |     wmma::fragment<wmma::matrix_a, WMMA_M, WMMA_N, WMMA_K, half, wmma::row_major> Q_mat;
      |           ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:65:26: warning: DPCT1082:74: Migration of nvcuda::wmma::matrix_a type is not supported.
   65 |     wmma::fragment<wmma::matrix_a, WMMA_M, WMMA_N, WMMA_K, half, wmma::row_major> Q_mat;
      |                          ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:65:72: warning: DPCT1082:75: Migration of nvcuda::wmma::row_major type is not supported.
   65 |     wmma::fragment<wmma::matrix_a, WMMA_M, WMMA_N, WMMA_K, half, wmma::row_major> Q_mat;
      |                                                                        ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:66:11: warning: DPCT1082:76: Migration of nvcuda::wmma::fragment<wmma::matrix_b, 16, 16, 16, half, wmma::col_major> type is not supported.
   66 |     wmma::fragment<wmma::matrix_b, WMMA_M, WMMA_N, WMMA_K, half, wmma::col_major> K_mat;
      |           ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:66:26: warning: DPCT1082:77: Migration of nvcuda::wmma::matrix_b type is not supported.
   66 |     wmma::fragment<wmma::matrix_b, WMMA_M, WMMA_N, WMMA_K, half, wmma::col_major> K_mat;
      |                          ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:66:72: warning: DPCT1082:78: Migration of nvcuda::wmma::col_major type is not supported.
   66 |     wmma::fragment<wmma::matrix_b, WMMA_M, WMMA_N, WMMA_K, half, wmma::col_major> K_mat;
      |                                                                        ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:67:11: warning: DPCT1082:79: Migration of nvcuda::wmma::fragment<wmma::accumulator, 16, 16, 16, half> type is not supported.
   67 |     wmma::fragment<wmma::accumulator, WMMA_M, WMMA_N, WMMA_K, half> QK_mat;
      |           ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:67:26: warning: DPCT1082:80: Migration of nvcuda::wmma::accumulator type is not supported.
   67 |     wmma::fragment<wmma::accumulator, WMMA_M, WMMA_N, WMMA_K, half> QK_mat;
      |                          ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:68:5: warning: DPCT1007:81: Migration of nvcuda::wmma::fill_fragment is not supported.
   68 |     wmma::fill_fragment(QK_mat, 0.0f);
      |     ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:77:7: warning: DPCT1007:82: Migration of nvcuda::wmma::mma_sync is not supported.
   77 |       wmma::mma_sync(QK_mat, Q_mat, K_mat, QK_mat);
      |       ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:137:11: warning: DPCT1082:83: Migration of nvcuda::wmma::fragment<wmma::matrix_a, 16, 16, 16, half, wmma::row_major> type is not supported.
  137 |     wmma::fragment<wmma::matrix_a, WMMA_M, WMMA_N, WMMA_K, half, wmma::row_major> Logits_mat;
      |           ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:137:26: warning: DPCT1082:84: Migration of nvcuda::wmma::matrix_a type is not supported.
  137 |     wmma::fragment<wmma::matrix_a, WMMA_M, WMMA_N, WMMA_K, half, wmma::row_major> Logits_mat;
      |                          ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:137:72: warning: DPCT1082:85: Migration of nvcuda::wmma::row_major type is not supported.
  137 |     wmma::fragment<wmma::matrix_a, WMMA_M, WMMA_N, WMMA_K, half, wmma::row_major> Logits_mat;
      |                                                                        ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:138:11: warning: DPCT1082:86: Migration of nvcuda::wmma::fragment<wmma::matrix_b, 16, 16, 16, half, wmma::row_major> type is not supported.
  138 |     wmma::fragment<wmma::matrix_b, WMMA_M, WMMA_N, WMMA_K, half, wmma::row_major> V_mat;
      |           ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:138:26: warning: DPCT1082:87: Migration of nvcuda::wmma::matrix_b type is not supported.
  138 |     wmma::fragment<wmma::matrix_b, WMMA_M, WMMA_N, WMMA_K, half, wmma::row_major> V_mat;
      |                          ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:138:72: warning: DPCT1082:88: Migration of nvcuda::wmma::row_major type is not supported.
  138 |     wmma::fragment<wmma::matrix_b, WMMA_M, WMMA_N, WMMA_K, half, wmma::row_major> V_mat;
      |                                                                        ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:139:11: warning: DPCT1082:89: Migration of nvcuda::wmma::fragment<wmma::accumulator, 16, 16, 16, half> type is not supported.
  139 |     wmma::fragment<wmma::accumulator, WMMA_M, WMMA_N, WMMA_K, half> QKV_mat;
      |           ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:139:26: warning: DPCT1082:90: Migration of nvcuda::wmma::accumulator type is not supported.
  139 |     wmma::fragment<wmma::accumulator, WMMA_M, WMMA_N, WMMA_K, half> QKV_mat;
      |                          ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:140:5: warning: DPCT1007:91: Migration of nvcuda::wmma::fill_fragment is not supported.
  140 |     wmma::fill_fragment(QKV_mat, 0.0f);
      |     ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:149:7: warning: DPCT1007:92: Migration of nvcuda::wmma::mma_sync is not supported.
  149 |       wmma::mma_sync(QKV_mat, Logits_mat, V_mat, QKV_mat);
      |       ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:79:5: warning: DPCT1007:93: Migration of nvcuda::wmma::store_matrix_sync is not supported.
   79 |     wmma::store_matrix_sync(s_logits[warp_from_offset] + warp_to_offset, QK_mat,
      |     ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:74:7: warning: DPCT1007:94: Migration of nvcuda::wmma::load_matrix_sync is not supported.
   74 |       wmma::load_matrix_sync(Q_mat, s_query[warp_from_offset] + k * WMMA_K,
      |       ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:76:7: warning: DPCT1007:95: Migration of nvcuda::wmma::load_matrix_sync is not supported.
   76 |       wmma::load_matrix_sync(K_mat, s_kv[warp_to_offset] + k * WMMA_K, size_per_head + SKEW_HALF);
      |       ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:151:5: warning: DPCT1007:96: Migration of nvcuda::wmma::store_matrix_sync is not supported.
  151 |     wmma::store_matrix_sync(s_query[warp_from_offset] + warp_to_offset, QKV_mat,
      |     ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:146:7: warning: DPCT1007:97: Migration of nvcuda::wmma::load_matrix_sync is not supported.
  146 |       wmma::load_matrix_sync(Logits_mat, s_logits[warp_from_offset] + k * WMMA_K,
      |       ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:148:7: warning: DPCT1007:98: Migration of nvcuda::wmma::load_matrix_sync is not supported.
  148 |       wmma::load_matrix_sync(V_mat, s_kv[k * WMMA_K] + warp_to_offset, size_per_head + SKEW_HALF);
      |       ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:164:1: warning: DPCT1110:99: The total declared local variable size in device function variety_wmma_attention_rm_kernel exceeds 128 bytes and may cause high register pressure. Consult with your hardware vendor to find the total register size available and adjust the code, or use smaller sub-group size to avoid high register pressure.
  164 | __global__
      | ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:199:11: warning: DPCT1082:100: Migration of nvcuda::wmma::fragment<wmma::matrix_a, 16, 16, 16, half, wmma::row_major> type is not supported.
  199 |     wmma::fragment<wmma::matrix_a, WMMA_M, WMMA_N, WMMA_K, half, wmma::row_major> Q_mat;
      |           ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:199:26: warning: DPCT1082:101: Migration of nvcuda::wmma::matrix_a type is not supported.
  199 |     wmma::fragment<wmma::matrix_a, WMMA_M, WMMA_N, WMMA_K, half, wmma::row_major> Q_mat;
      |                          ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:199:72: warning: DPCT1082:102: Migration of nvcuda::wmma::row_major type is not supported.
  199 |     wmma::fragment<wmma::matrix_a, WMMA_M, WMMA_N, WMMA_K, half, wmma::row_major> Q_mat;
      |                                                                        ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:200:11: warning: DPCT1082:103: Migration of nvcuda::wmma::fragment<wmma::matrix_b, 16, 16, 16, half, wmma::col_major> type is not supported.
  200 |     wmma::fragment<wmma::matrix_b, WMMA_M, WMMA_N, WMMA_K, half, wmma::col_major> K_mat;
      |           ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:200:26: warning: DPCT1082:104: Migration of nvcuda::wmma::matrix_b type is not supported.
  200 |     wmma::fragment<wmma::matrix_b, WMMA_M, WMMA_N, WMMA_K, half, wmma::col_major> K_mat;
      |                          ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:200:72: warning: DPCT1082:105: Migration of nvcuda::wmma::col_major type is not supported.
  200 |     wmma::fragment<wmma::matrix_b, WMMA_M, WMMA_N, WMMA_K, half, wmma::col_major> K_mat;
      |                                                                        ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:201:11: warning: DPCT1082:106: Migration of nvcuda::wmma::fragment<wmma::accumulator, 16, 16, 16, half> type is not supported.
  201 |     wmma::fragment<wmma::accumulator, WMMA_M, WMMA_N, WMMA_K, half> QK_mat;
      |           ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:201:26: warning: DPCT1082:107: Migration of nvcuda::wmma::accumulator type is not supported.
  201 |     wmma::fragment<wmma::accumulator, WMMA_M, WMMA_N, WMMA_K, half> QK_mat;
      |                          ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:202:5: warning: DPCT1007:108: Migration of nvcuda::wmma::fill_fragment is not supported.
  202 |     wmma::fill_fragment(QK_mat, 0.0f);
      |     ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:211:7: warning: DPCT1007:109: Migration of nvcuda::wmma::mma_sync is not supported.
  211 |       wmma::mma_sync(QK_mat, Q_mat, K_mat, QK_mat);
      |       ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:268:11: warning: DPCT1082:110: Migration of nvcuda::wmma::fragment<wmma::matrix_a, 16, 16, 16, half, wmma::row_major> type is not supported.
  268 |     wmma::fragment<wmma::matrix_a, WMMA_M, WMMA_N, WMMA_K, half, wmma::row_major> Logits_mat;
      |           ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:268:26: warning: DPCT1082:111: Migration of nvcuda::wmma::matrix_a type is not supported.
  268 |     wmma::fragment<wmma::matrix_a, WMMA_M, WMMA_N, WMMA_K, half, wmma::row_major> Logits_mat;
      |                          ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:268:72: warning: DPCT1082:112: Migration of nvcuda::wmma::row_major type is not supported.
  268 |     wmma::fragment<wmma::matrix_a, WMMA_M, WMMA_N, WMMA_K, half, wmma::row_major> Logits_mat;
      |                                                                        ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:269:11: warning: DPCT1082:113: Migration of nvcuda::wmma::fragment<wmma::matrix_b, 16, 16, 16, half, wmma::row_major> type is not supported.
  269 |     wmma::fragment<wmma::matrix_b, WMMA_M, WMMA_N, WMMA_K, half, wmma::row_major> V_mat;
      |           ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:269:26: warning: DPCT1082:114: Migration of nvcuda::wmma::matrix_b type is not supported.
  269 |     wmma::fragment<wmma::matrix_b, WMMA_M, WMMA_N, WMMA_K, half, wmma::row_major> V_mat;
      |                          ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:269:72: warning: DPCT1082:115: Migration of nvcuda::wmma::row_major type is not supported.
  269 |     wmma::fragment<wmma::matrix_b, WMMA_M, WMMA_N, WMMA_K, half, wmma::row_major> V_mat;
      |                                                                        ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:270:11: warning: DPCT1082:116: Migration of nvcuda::wmma::fragment<wmma::accumulator, 16, 16, 16, half> type is not supported.
  270 |     wmma::fragment<wmma::accumulator, WMMA_M, WMMA_N, WMMA_K, half> QKV_mat;
      |           ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:270:26: warning: DPCT1082:117: Migration of nvcuda::wmma::accumulator type is not supported.
  270 |     wmma::fragment<wmma::accumulator, WMMA_M, WMMA_N, WMMA_K, half> QKV_mat;
      |                          ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:271:5: warning: DPCT1007:118: Migration of nvcuda::wmma::fill_fragment is not supported.
  271 |     wmma::fill_fragment(QKV_mat, 0.0f);
      |     ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:280:7: warning: DPCT1007:119: Migration of nvcuda::wmma::mma_sync is not supported.
  280 |       wmma::mma_sync(QKV_mat, Logits_mat, V_mat, QKV_mat);
      |       ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:213:5: warning: DPCT1007:120: Migration of nvcuda::wmma::store_matrix_sync is not supported.
  213 |     wmma::store_matrix_sync(s_logits[warp_from_offset] + warp_to_offset, QK_mat,
      |     ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:208:7: warning: DPCT1007:121: Migration of nvcuda::wmma::load_matrix_sync is not supported.
  208 |       wmma::load_matrix_sync(Q_mat, s_query[warp_from_offset] + k * WMMA_K,
      |       ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:210:7: warning: DPCT1007:122: Migration of nvcuda::wmma::load_matrix_sync is not supported.
  210 |       wmma::load_matrix_sync(K_mat, s_kv[warp_to_offset] + k * WMMA_K, size_per_head + SKEW_HALF);
      |       ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:282:5: warning: DPCT1007:123: Migration of nvcuda::wmma::store_matrix_sync is not supported.
  282 |     wmma::store_matrix_sync(s_query[warp_from_offset] + warp_to_offset, QKV_mat,
      |     ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:277:7: warning: DPCT1007:124: Migration of nvcuda::wmma::load_matrix_sync is not supported.
  277 |       wmma::load_matrix_sync(Logits_mat, s_logits[warp_from_offset] + k * WMMA_K,
      |       ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:279:7: warning: DPCT1007:125: Migration of nvcuda::wmma::load_matrix_sync is not supported.
  279 |       wmma::load_matrix_sync(V_mat, s_kv[k * WMMA_K] + warp_to_offset, size_per_head + SKEW_HALF);
      |       ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:295:1: warning: DPCT1110:126: The total declared local variable size in device function variety_wmma_attention_long_kernel exceeds 128 bytes and may cause high register pressure. Consult with your hardware vendor to find the total register size available and adjust the code, or use smaller sub-group size to avoid high register pressure.
  295 | __global__ void variety_wmma_attention_long_kernel(
      | ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:339:11: warning: DPCT1082:127: Migration of nvcuda::wmma::fragment<wmma::matrix_a, 16, 16, 16, half, wmma::row_major> type is not supported.
  339 |     wmma::fragment<wmma::matrix_a, WMMA_M, WMMA_N, WMMA_K, half, wmma::row_major> Q_mat;
      |           ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:339:26: warning: DPCT1082:128: Migration of nvcuda::wmma::matrix_a type is not supported.
  339 |     wmma::fragment<wmma::matrix_a, WMMA_M, WMMA_N, WMMA_K, half, wmma::row_major> Q_mat;
      |                          ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:339:72: warning: DPCT1082:129: Migration of nvcuda::wmma::row_major type is not supported.
  339 |     wmma::fragment<wmma::matrix_a, WMMA_M, WMMA_N, WMMA_K, half, wmma::row_major> Q_mat;
      |                                                                        ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:340:11: warning: DPCT1082:130: Migration of nvcuda::wmma::fragment<wmma::matrix_b, 16, 16, 16, half, wmma::col_major> type is not supported.
  340 |     wmma::fragment<wmma::matrix_b, WMMA_M, WMMA_N, WMMA_K, half, wmma::col_major> K_mat;
      |           ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:340:26: warning: DPCT1082:131: Migration of nvcuda::wmma::matrix_b type is not supported.
  340 |     wmma::fragment<wmma::matrix_b, WMMA_M, WMMA_N, WMMA_K, half, wmma::col_major> K_mat;
      |                          ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:340:72: warning: DPCT1082:132: Migration of nvcuda::wmma::col_major type is not supported.
  340 |     wmma::fragment<wmma::matrix_b, WMMA_M, WMMA_N, WMMA_K, half, wmma::col_major> K_mat;
      |                                                                        ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:341:11: warning: DPCT1082:133: Migration of nvcuda::wmma::fragment<wmma::accumulator, 16, 16, 16, half> type is not supported.
  341 |     wmma::fragment<wmma::accumulator, WMMA_M, WMMA_N, WMMA_K, half> QK_mat;
      |           ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:341:26: warning: DPCT1082:134: Migration of nvcuda::wmma::accumulator type is not supported.
  341 |     wmma::fragment<wmma::accumulator, WMMA_M, WMMA_N, WMMA_K, half> QK_mat;
      |                          ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:342:5: warning: DPCT1007:135: Migration of nvcuda::wmma::fill_fragment is not supported.
  342 |     wmma::fill_fragment(QK_mat, 0.0f);
      |     ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:351:7: warning: DPCT1007:136: Migration of nvcuda::wmma::mma_sync is not supported.
  351 |       wmma::mma_sync(QK_mat, Q_mat, K_mat, QK_mat);
      |       ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:417:11: warning: DPCT1082:137: Migration of nvcuda::wmma::fragment<wmma::matrix_a, 16, 16, 16, half, wmma::row_major> type is not supported.
  417 |     wmma::fragment<wmma::matrix_a, WMMA_M, WMMA_N, WMMA_K, half, wmma::row_major> Logits_mat;
      |           ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:417:26: warning: DPCT1082:138: Migration of nvcuda::wmma::matrix_a type is not supported.
  417 |     wmma::fragment<wmma::matrix_a, WMMA_M, WMMA_N, WMMA_K, half, wmma::row_major> Logits_mat;
      |                          ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:417:72: warning: DPCT1082:139: Migration of nvcuda::wmma::row_major type is not supported.
  417 |     wmma::fragment<wmma::matrix_a, WMMA_M, WMMA_N, WMMA_K, half, wmma::row_major> Logits_mat;
      |                                                                        ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:418:11: warning: DPCT1082:140: Migration of nvcuda::wmma::fragment<wmma::matrix_b, 16, 16, 16, half, wmma::row_major> type is not supported.
  418 |     wmma::fragment<wmma::matrix_b, WMMA_M, WMMA_N, WMMA_K, half, wmma::row_major> V_mat;
      |           ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:418:26: warning: DPCT1082:141: Migration of nvcuda::wmma::matrix_b type is not supported.
  418 |     wmma::fragment<wmma::matrix_b, WMMA_M, WMMA_N, WMMA_K, half, wmma::row_major> V_mat;
      |                          ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:418:72: warning: DPCT1082:142: Migration of nvcuda::wmma::row_major type is not supported.
  418 |     wmma::fragment<wmma::matrix_b, WMMA_M, WMMA_N, WMMA_K, half, wmma::row_major> V_mat;
      |                                                                        ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:419:11: warning: DPCT1082:143: Migration of nvcuda::wmma::fragment<wmma::accumulator, 16, 16, 16, half> type is not supported.
  419 |     wmma::fragment<wmma::accumulator, WMMA_M, WMMA_N, WMMA_K, half> QKV_mat;
      |           ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:419:26: warning: DPCT1082:144: Migration of nvcuda::wmma::accumulator type is not supported.
  419 |     wmma::fragment<wmma::accumulator, WMMA_M, WMMA_N, WMMA_K, half> QKV_mat;
      |                          ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:420:5: warning: DPCT1007:145: Migration of nvcuda::wmma::fill_fragment is not supported.
  420 |     wmma::fill_fragment(QKV_mat, 0.0f);
      |     ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:429:7: warning: DPCT1007:146: Migration of nvcuda::wmma::mma_sync is not supported.
  429 |       wmma::mma_sync(QKV_mat, Logits_mat, V_mat, QKV_mat);
      |       ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:353:5: warning: DPCT1007:147: Migration of nvcuda::wmma::store_matrix_sync is not supported.
  353 |     wmma::store_matrix_sync(s_logits[warp_from_offset] + warp_to_offset, QK_mat,
      |     ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:348:7: warning: DPCT1007:148: Migration of nvcuda::wmma::load_matrix_sync is not supported.
  348 |       wmma::load_matrix_sync(Q_mat, s_query[warp_from_offset] + k * WMMA_K,
      |       ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:350:7: warning: DPCT1007:149: Migration of nvcuda::wmma::load_matrix_sync is not supported.
  350 |       wmma::load_matrix_sync(K_mat, s_kv[warp_to_offset] + k * WMMA_K, size_per_head + SKEW_HALF);
      |       ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:431:5: warning: DPCT1007:150: Migration of nvcuda::wmma::store_matrix_sync is not supported.
  431 |     wmma::store_matrix_sync(s_query[warp_from_offset] + warp_to_offset, QKV_mat,
      |     ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:426:7: warning: DPCT1007:151: Migration of nvcuda::wmma::load_matrix_sync is not supported.
  426 |       wmma::load_matrix_sync(Logits_mat, s_logits[warp_from_offset] + k * WMMA_K,
      |       ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:428:7: warning: DPCT1007:152: Migration of nvcuda::wmma::load_matrix_sync is not supported.
  428 |       wmma::load_matrix_sync(V_mat, s_kv[k * WMMA_K] + warp_to_offset, size_per_head + SKEW_HALF);
      |       ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:444:1: warning: DPCT1110:153: The total declared local variable size in device function variety_wmma_attention_long_rm_kernel exceeds 128 bytes and may cause high register pressure. Consult with your hardware vendor to find the total register size available and adjust the code, or use smaller sub-group size to avoid high register pressure.
  444 | __global__ void variety_wmma_attention_long_rm_kernel(
      | ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:493:11: warning: DPCT1082:154: Migration of nvcuda::wmma::fragment<wmma::matrix_a, 16, 16, 16, half, wmma::row_major> type is not supported.
  493 |     wmma::fragment<wmma::matrix_a, WMMA_M, WMMA_N, WMMA_K, half, wmma::row_major> Q_mat;
      |           ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:493:26: warning: DPCT1082:155: Migration of nvcuda::wmma::matrix_a type is not supported.
  493 |     wmma::fragment<wmma::matrix_a, WMMA_M, WMMA_N, WMMA_K, half, wmma::row_major> Q_mat;
      |                          ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:493:72: warning: DPCT1082:156: Migration of nvcuda::wmma::row_major type is not supported.
  493 |     wmma::fragment<wmma::matrix_a, WMMA_M, WMMA_N, WMMA_K, half, wmma::row_major> Q_mat;
      |                                                                        ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:494:11: warning: DPCT1082:157: Migration of nvcuda::wmma::fragment<wmma::matrix_b, 16, 16, 16, half, wmma::col_major> type is not supported.
  494 |     wmma::fragment<wmma::matrix_b, WMMA_M, WMMA_N, WMMA_K, half, wmma::col_major> K_mat;
      |           ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:494:26: warning: DPCT1082:158: Migration of nvcuda::wmma::matrix_b type is not supported.
  494 |     wmma::fragment<wmma::matrix_b, WMMA_M, WMMA_N, WMMA_K, half, wmma::col_major> K_mat;
      |                          ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:494:72: warning: DPCT1082:159: Migration of nvcuda::wmma::col_major type is not supported.
  494 |     wmma::fragment<wmma::matrix_b, WMMA_M, WMMA_N, WMMA_K, half, wmma::col_major> K_mat;
      |                                                                        ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:495:11: warning: DPCT1082:160: Migration of nvcuda::wmma::fragment<wmma::accumulator, 16, 16, 16, half> type is not supported.
  495 |     wmma::fragment<wmma::accumulator, WMMA_M, WMMA_N, WMMA_K, half> QK_mat;
      |           ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:495:26: warning: DPCT1082:161: Migration of nvcuda::wmma::accumulator type is not supported.
  495 |     wmma::fragment<wmma::accumulator, WMMA_M, WMMA_N, WMMA_K, half> QK_mat;
      |                          ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:496:5: warning: DPCT1007:162: Migration of nvcuda::wmma::fill_fragment is not supported.
  496 |     wmma::fill_fragment(QK_mat, 0.0f);
      |     ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:507:9: warning: DPCT1007:163: Migration of nvcuda::wmma::mma_sync is not supported.
  507 |         wmma::mma_sync(QK_mat, Q_mat, K_mat, QK_mat);
      |         ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:571:11: warning: DPCT1082:164: Migration of nvcuda::wmma::fragment<wmma::matrix_a, 16, 16, 16, half, wmma::row_major> type is not supported.
  571 |     wmma::fragment<wmma::matrix_a, WMMA_M, WMMA_N, WMMA_K, half, wmma::row_major> Logits_mat;
      |           ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:571:26: warning: DPCT1082:165: Migration of nvcuda::wmma::matrix_a type is not supported.
  571 |     wmma::fragment<wmma::matrix_a, WMMA_M, WMMA_N, WMMA_K, half, wmma::row_major> Logits_mat;
      |                          ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:571:72: warning: DPCT1082:166: Migration of nvcuda::wmma::row_major type is not supported.
  571 |     wmma::fragment<wmma::matrix_a, WMMA_M, WMMA_N, WMMA_K, half, wmma::row_major> Logits_mat;
      |                                                                        ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:572:11: warning: DPCT1082:167: Migration of nvcuda::wmma::fragment<wmma::matrix_b, 16, 16, 16, half, wmma::row_major> type is not supported.
  572 |     wmma::fragment<wmma::matrix_b, WMMA_M, WMMA_N, WMMA_K, half, wmma::row_major> V_mat;
      |           ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:572:26: warning: DPCT1082:168: Migration of nvcuda::wmma::matrix_b type is not supported.
  572 |     wmma::fragment<wmma::matrix_b, WMMA_M, WMMA_N, WMMA_K, half, wmma::row_major> V_mat;
      |                          ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:572:72: warning: DPCT1082:169: Migration of nvcuda::wmma::row_major type is not supported.
  572 |     wmma::fragment<wmma::matrix_b, WMMA_M, WMMA_N, WMMA_K, half, wmma::row_major> V_mat;
      |                                                                        ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:573:11: warning: DPCT1082:170: Migration of nvcuda::wmma::fragment<wmma::accumulator, 16, 16, 16, half> type is not supported.
  573 |     wmma::fragment<wmma::accumulator, WMMA_M, WMMA_N, WMMA_K, half> QKV_mat;
      |           ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:573:26: warning: DPCT1082:171: Migration of nvcuda::wmma::accumulator type is not supported.
  573 |     wmma::fragment<wmma::accumulator, WMMA_M, WMMA_N, WMMA_K, half> QKV_mat;
      |                          ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:574:5: warning: DPCT1007:172: Migration of nvcuda::wmma::fill_fragment is not supported.
  574 |     wmma::fill_fragment(QKV_mat, 0.0f);
      |     ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:583:7: warning: DPCT1007:173: Migration of nvcuda::wmma::mma_sync is not supported.
  583 |       wmma::mma_sync(QKV_mat, Logits_mat, V_mat, QKV_mat);
      |       ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:509:7: warning: DPCT1007:174: Migration of nvcuda::wmma::store_matrix_sync is not supported.
  509 |       wmma::store_matrix_sync(s_logits[warp_from_offset] + warp_to_offset, QK_mat,
      |       ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:503:9: warning: DPCT1007:175: Migration of nvcuda::wmma::load_matrix_sync is not supported.
  503 |         wmma::load_matrix_sync(Q_mat, s_query[warp_from_offset] + k * WMMA_K,
      |         ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:505:9: warning: DPCT1007:176: Migration of nvcuda::wmma::load_matrix_sync is not supported.
  505 |         wmma::load_matrix_sync(K_mat, s_kv[warp_to_offset] + k * WMMA_K,
      |         ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:585:5: warning: DPCT1007:177: Migration of nvcuda::wmma::store_matrix_sync is not supported.
  585 |     wmma::store_matrix_sync(s_query[warp_from_offset] + warp_to_offset, QKV_mat,
      |     ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:580:7: warning: DPCT1007:178: Migration of nvcuda::wmma::load_matrix_sync is not supported.
  580 |       wmma::load_matrix_sync(Logits_mat, s_logits[warp_from_offset] + k * WMMA_K,
      |       ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:582:7: warning: DPCT1007:179: Migration of nvcuda::wmma::load_matrix_sync is not supported.
  582 |       wmma::load_matrix_sync(V_mat, s_kv[k * WMMA_K] + warp_to_offset, size_per_head + SKEW_HALF);
      |       ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:598:3: warning: DPCT1049:180: The work-group size passed to the SYCL kernel may exceed the limit. To get the device limit, query info::device::max_work_group_size. Adjust the work-group size if needed.
  598 |   variety_wmma_attention_kernel<SEQ_LEN, SIZE_PER_HEAD><<<grid, block, 0, stream>>>( \
      |   ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:602:3: warning: DPCT1049:181: The work-group size passed to the SYCL kernel may exceed the limit. To get the device limit, query info::device::max_work_group_size. Adjust the work-group size if needed.
  602 |   variety_wmma_attention_rm_kernel<SEQ_LEN, SIZE_PER_HEAD>                                \
      |   ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:615:3: warning: DPCT1049:182: The work-group size passed to the SYCL kernel may exceed the limit. To get the device limit, query info::device::max_work_group_size. Adjust the work-group size if needed.
  615 |   variety_wmma_attention_long_kernel<SEQ_LEN, SIZE_PER_HEAD, SPLIT_LEN>                           \
      |   ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:630:3: warning: DPCT1049:183: The work-group size passed to the SYCL kernel may exceed the limit. To get the device limit, query info::device::max_work_group_size. Adjust the work-group size if needed.
  630 |   variety_wmma_attention_long_rm_kernel<SEQ_LEN, SIZE_PER_HEAD, SPLIT_LEN>                        \
      |   ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_nofused.cu:57:5: warning: DPCT1049:184: The work-group size passed to the SYCL kernel may exceed the limit. To get the device limit, query info::device::max_work_group_size. Adjust the work-group size if needed.
   57 |     add_QKV_bias_padding<<<grid, block, 0, stream>>>(  // restore & clean zero
      |     ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_nofused.cu:63:5: warning: DPCT1049:185: The work-group size passed to the SYCL kernel may exceed the limit. To get the device limit, query info::device::max_work_group_size. Adjust the work-group size if needed.
   63 |     add_QKV_bias<<<grid, block, 0, stream>>>(
      |     ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_nofused.cu:106:7: warning: DPCT1049:186: The work-group size passed to the SYCL kernel may exceed the limit. To get the device limit, query info::device::max_work_group_size. Adjust the work-group size if needed.
  106 |       transpose_rm_padding<<<et_param.valid_word_num, block, 0, stream>>>(
      |       ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_nofused.cu:110:7: warning: DPCT1049:187: The work-group size passed to the SYCL kernel may exceed the limit. To get the device limit, query info::device::max_work_group_size. Adjust the work-group size if needed.
  110 |       transpose<<<batch_size * seq_len, block, 0, stream>>>(
      |       ^
In file included from /export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/cutlass_attention.cu:24:
In file included from /export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/cutlass_attention_defs.h:33:
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/cutlass_attention_operators.h:289:3: warning: DPCT1110:188: The total declared local variable size in device function operator() exceeds 128 bytes and may cause high register pressure. Consult with your hardware vendor to find the total register size available and adjust the code, or use smaller sub-group size to avoid high register pressure.
  289 |   CUTLASS_HOST_DEVICE typename Iterator::Fragment operator()(
      |   ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/cutlass_attention_operators.h:419:3: warning: DPCT1049:189: The work-group size passed to the SYCL kernel may exceed the limit. To get the device limit, query info::device::max_work_group_size. Adjust the work-group size if needed.
  419 |   softmax_reduction_kernel<T, ThreadblockN>
      |   ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/cutlass_attention.cu:187:7: warning: DPCT1049:190: The work-group size passed to the SYCL kernel may exceed the limit. To get the device limit, query info::device::max_work_group_size. Adjust the work-group size if needed.
  187 |       add_QKV_bias_padding<<<grid, block, 0, stream>>>(  // restore & clean zero for batch_gemm
      |       ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/cutlass_attention.cu:191:7: warning: DPCT1049:191: The work-group size passed to the SYCL kernel may exceed the limit. To get the device limit, query info::device::max_work_group_size. Adjust the work-group size if needed.
  191 |       add_QKV_bias<<<grid, block, 0, stream>>>(infer_param->qkv, this->param_.attr_bias_QKV, query,
      |       ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/layernorm.h:389:5: warning: DPCT1049:192: The work-group size passed to the SYCL kernel may exceed the limit. To get the device limit, query info::device::max_work_group_size. Adjust the work-group size if needed.
  389 |     input_layernorm<<<grid, block, 0, stream>>>(output, input, gamma, beta, n, use_fp32);
      |     ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/layernorm.h:383:7: warning: DPCT1049:193: The work-group size passed to the SYCL kernel may exceed the limit. To get the device limit, query info::device::max_work_group_size. Adjust the work-group size if needed.
  383 |       input_layernorm_v2<2>
      |       ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/layernorm.h:386:7: warning: DPCT1049:194: The work-group size passed to the SYCL kernel may exceed the limit. To get the device limit, query info::device::max_work_group_size. Adjust the work-group size if needed.
  386 |       input_layernorm_v2<4>
      |       ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/layernorm.h:406:5: warning: DPCT1049:195: The work-group size passed to the SYCL kernel may exceed the limit. To get the device limit, query info::device::max_work_group_size. Adjust the work-group size if needed.
  406 |     input_compress_layernorm<<<grid, block, 0, stream>>>(output2, input, gamma, beta, n, use_fp32,
      |     ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/layernorm.h:400:7: warning: DPCT1049:196: The work-group size passed to the SYCL kernel may exceed the limit. To get the device limit, query info::device::max_work_group_size. Adjust the work-group size if needed.
  400 |       input_compress_layernorm_v2<2><<<grid, block.x / 2, 0, stream>>>(
      |       ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/layernorm.h:403:7: warning: DPCT1049:197: The work-group size passed to the SYCL kernel may exceed the limit. To get the device limit, query info::device::max_work_group_size. Adjust the work-group size if needed.
  403 |       input_compress_layernorm_v2<4><<<grid, block.x / 4, 0, stream>>>(
      |       ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/layernorm.h:417:7: warning: DPCT1049:198: The work-group size passed to the SYCL kernel may exceed the limit. To get the device limit, query info::device::max_work_group_size. Adjust the work-group size if needed.
  417 |       add_bias_input_layernorm_v2<2>
      |       ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/layernorm.h:420:7: warning: DPCT1049:199: The work-group size passed to the SYCL kernel may exceed the limit. To get the device limit, query info::device::max_work_group_size. Adjust the work-group size if needed.
  420 |       add_bias_input_layernorm_v2<4>
      |       ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/layernorm.h:425:5: warning: DPCT1049:200: The work-group size passed to the SYCL kernel may exceed the limit. To get the device limit, query info::device::max_work_group_size. Adjust the work-group size if needed.
  425 |     add_bias_input_layernorm<<<grid, block, 0, stream>>>(output, input, bias, gamma, beta, n,
      |     ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/layernorm.h:444:5: warning: DPCT1049:201: The work-group size passed to the SYCL kernel may exceed the limit. To get the device limit, query info::device::max_work_group_size. Adjust the work-group size if needed.
  444 |     add_bias_input_layernorm_restore_output<<<grid, block, 0, stream>>>(
      |     ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/layernorm.h:438:7: warning: DPCT1049:202: The work-group size passed to the SYCL kernel may exceed the limit. To get the device limit, query info::device::max_work_group_size. Adjust the work-group size if needed.
  438 |       add_bias_input_layernorm_restore_output_v2<2><<<grid, block.x / 2, 0, stream>>>(
      |       ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/layernorm.h:441:7: warning: DPCT1049:203: The work-group size passed to the SYCL kernel may exceed the limit. To get the device limit, query info::device::max_work_group_size. Adjust the work-group size if needed.
  441 |       add_bias_input_layernorm_restore_output_v2<4><<<grid, block.x / 4, 0, stream>>>(
      |       ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/layernorm.h:518:5: warning: DPCT1049:204: The work-group size passed to the SYCL kernel may exceed the limit. To get the device limit, query info::device::max_work_group_size. Adjust the work-group size if needed.
  518 |     add_bias_input_out_layernorm<<<grid, block, 0, stream>>>(output, input, bias, output2, gamma,
      |     ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/layernorm.h:512:7: warning: DPCT1049:205: The work-group size passed to the SYCL kernel may exceed the limit. To get the device limit, query info::device::max_work_group_size. Adjust the work-group size if needed.
  512 |       add_bias_input_out_layernorm_v2<2><<<grid, block.x / 2, 0, stream>>>(
      |       ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/layernorm.h:515:7: warning: DPCT1049:206: The work-group size passed to the SYCL kernel may exceed the limit. To get the device limit, query info::device::max_work_group_size. Adjust the work-group size if needed.
  515 |       add_bias_input_out_layernorm_v2<4><<<grid, block.x / 4, 0, stream>>>(
      |       ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/layernorm.h:723:5: warning: DPCT1049:207: The work-group size passed to the SYCL kernel may exceed the limit. To get the device limit, query info::device::max_work_group_size. Adjust the work-group size if needed.
  723 |     add_bias_half_input_layernorm<<<grid, block, 0, stream>>>(output, input, bias, gamma, beta, n,
      |     ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/layernorm.h:717:7: warning: DPCT1049:208: The work-group size passed to the SYCL kernel may exceed the limit. To get the device limit, query info::device::max_work_group_size. Adjust the work-group size if needed.
  717 |       add_bias_half_input_layernorm_v2<2>
      |       ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/layernorm.h:720:7: warning: DPCT1049:209: The work-group size passed to the SYCL kernel may exceed the limit. To get the device limit, query info::device::max_work_group_size. Adjust the work-group size if needed.
  720 |       add_bias_half_input_layernorm_v2<4>
      |       ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/layernorm.h:741:5: warning: DPCT1049:210: The work-group size passed to the SYCL kernel may exceed the limit. To get the device limit, query info::device::max_work_group_size. Adjust the work-group size if needed.
  741 |     add_bias_half_input_layernorm_restore_output<<<grid, block, 0, stream>>>(
      |     ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/layernorm.h:735:7: warning: DPCT1049:211: The work-group size passed to the SYCL kernel may exceed the limit. To get the device limit, query info::device::max_work_group_size. Adjust the work-group size if needed.
  735 |       add_bias_half_input_layernorm_restore_output_v2<2><<<grid, block.x / 2, 0, stream>>>(
      |       ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/layernorm.h:738:7: warning: DPCT1049:212: The work-group size passed to the SYCL kernel may exceed the limit. To get the device limit, query info::device::max_work_group_size. Adjust the work-group size if needed.
  738 |       add_bias_half_input_layernorm_restore_output_v2<4><<<grid, block.x / 4, 0, stream>>>(
      |       ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/layernorm.h:760:5: warning: DPCT1049:213: The work-group size passed to the SYCL kernel may exceed the limit. To get the device limit, query info::device::max_work_group_size. Adjust the work-group size if needed.
  760 |     add_bias_half_input_out_layernorm<<<grid, block, 0, stream>>>(output, input, bias, output2,
      |     ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/layernorm.h:754:7: warning: DPCT1049:214: The work-group size passed to the SYCL kernel may exceed the limit. To get the device limit, query info::device::max_work_group_size. Adjust the work-group size if needed.
  754 |       add_bias_half_input_out_layernorm_v2<2><<<grid, block.x / 2, 0, stream>>>(
      |       ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/layernorm.h:757:7: warning: DPCT1049:215: The work-group size passed to the SYCL kernel may exceed the limit. To get the device limit, query info::device::max_work_group_size. Adjust the work-group size if needed.
  757 |       add_bias_half_input_out_layernorm_v2<4><<<grid, block.x / 4, 0, stream>>>(
      |       ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/remove_padding.h:97:3: warning: DPCT1049:216: The work-group size passed to the SYCL kernel may exceed the limit. To get the device limit, query info::device::max_work_group_size. Adjust the work-group size if needed.
   97 |   parallel_prefix<<<1, block, (2 * batch_size + 1) * sizeof(int), stream>>>(
      |   ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/remove_padding.h:119:3: warning: DPCT1049:217: The work-group size passed to the SYCL kernel may exceed the limit. To get the device limit, query info::device::max_work_group_size. Adjust the work-group size if needed.
  119 |   compress_bert_input<<<grid, block, 0, stream>>>(from_tensor, to_tensor, batch_idx, word_idx,
      |   ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused.cu:33:1: warning: DPCT1110:218: The total declared local variable size in device function wmma_attention_kernel_16 exceeds 128 bytes and may cause high register pressure. Consult with your hardware vendor to find the total register size available and adjust the code, or use smaller sub-group size to avoid high register pressure.
   33 | __global__
      | ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused.cu:69:11: warning: DPCT1082:219: Migration of nvcuda::wmma::fragment<wmma::matrix_a, 16, 16, 16, half, wmma::row_major> type is not supported.
   69 |     wmma::fragment<wmma::matrix_a, WMMA_M, WMMA_N, WMMA_K, half, wmma::row_major> Q_mat;
      |           ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused.cu:69:26: warning: DPCT1082:220: Migration of nvcuda::wmma::matrix_a type is not supported.
   69 |     wmma::fragment<wmma::matrix_a, WMMA_M, WMMA_N, WMMA_K, half, wmma::row_major> Q_mat;
      |                          ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused.cu:69:72: warning: DPCT1082:221: Migration of nvcuda::wmma::row_major type is not supported.
   69 |     wmma::fragment<wmma::matrix_a, WMMA_M, WMMA_N, WMMA_K, half, wmma::row_major> Q_mat;
      |                                                                        ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused.cu:70:11: warning: DPCT1082:222: Migration of nvcuda::wmma::fragment<wmma::matrix_b, 16, 16, 16, half, wmma::col_major> type is not supported.
   70 |     wmma::fragment<wmma::matrix_b, WMMA_M, WMMA_N, WMMA_K, half, wmma::col_major> K_mat;
      |           ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused.cu:70:26: warning: DPCT1082:223: Migration of nvcuda::wmma::matrix_b type is not supported.
   70 |     wmma::fragment<wmma::matrix_b, WMMA_M, WMMA_N, WMMA_K, half, wmma::col_major> K_mat;
      |                          ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused.cu:70:72: warning: DPCT1082:224: Migration of nvcuda::wmma::col_major type is not supported.
   70 |     wmma::fragment<wmma::matrix_b, WMMA_M, WMMA_N, WMMA_K, half, wmma::col_major> K_mat;
      |                                                                        ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused.cu:71:11: warning: DPCT1082:225: Migration of nvcuda::wmma::fragment<wmma::accumulator, 16, 16, 16, half> type is not supported.
   71 |     wmma::fragment<wmma::accumulator, WMMA_M, WMMA_N, WMMA_K, half> QK_mat;
      |           ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused.cu:71:26: warning: DPCT1082:226: Migration of nvcuda::wmma::accumulator type is not supported.
   71 |     wmma::fragment<wmma::accumulator, WMMA_M, WMMA_N, WMMA_K, half> QK_mat;
      |                          ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused.cu:72:5: warning: DPCT1007:227: Migration of nvcuda::wmma::fill_fragment is not supported.
   72 |     wmma::fill_fragment(QK_mat, 0.0f);
      |     ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused.cu:81:7: warning: DPCT1007:228: Migration of nvcuda::wmma::mma_sync is not supported.
   81 |       wmma::mma_sync(QK_mat, Q_mat, K_mat, QK_mat);
      |       ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused.cu:142:11: warning: DPCT1082:229: Migration of nvcuda::wmma::fragment<wmma::matrix_a, 16, 16, 16, half, wmma::row_major> type is not supported.
  142 |     wmma::fragment<wmma::matrix_a, WMMA_M, WMMA_N, WMMA_K, half, wmma::row_major> Logits_mat;
      |           ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused.cu:142:26: warning: DPCT1082:230: Migration of nvcuda::wmma::matrix_a type is not supported.
  142 |     wmma::fragment<wmma::matrix_a, WMMA_M, WMMA_N, WMMA_K, half, wmma::row_major> Logits_mat;
      |                          ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused.cu:142:72: warning: DPCT1082:231: Migration of nvcuda::wmma::row_major type is not supported.
  142 |     wmma::fragment<wmma::matrix_a, WMMA_M, WMMA_N, WMMA_K, half, wmma::row_major> Logits_mat;
      |                                                                        ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused.cu:143:11: warning: DPCT1082:232: Migration of nvcuda::wmma::fragment<wmma::matrix_b, 16, 16, 16, half, wmma::row_major> type is not supported.
  143 |     wmma::fragment<wmma::matrix_b, WMMA_M, WMMA_N, WMMA_K, half, wmma::row_major> V_mat;
      |           ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused.cu:143:26: warning: DPCT1082:233: Migration of nvcuda::wmma::matrix_b type is not supported.
  143 |     wmma::fragment<wmma::matrix_b, WMMA_M, WMMA_N, WMMA_K, half, wmma::row_major> V_mat;
      |                          ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused.cu:143:72: warning: DPCT1082:234: Migration of nvcuda::wmma::row_major type is not supported.
  143 |     wmma::fragment<wmma::matrix_b, WMMA_M, WMMA_N, WMMA_K, half, wmma::row_major> V_mat;
      |                                                                        ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused.cu:144:11: warning: DPCT1082:235: Migration of nvcuda::wmma::fragment<wmma::accumulator, 16, 16, 16, half> type is not supported.
  144 |     wmma::fragment<wmma::accumulator, WMMA_M, WMMA_N, WMMA_K, half> QKV_mat;
      |           ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused.cu:144:26: warning: DPCT1082:236: Migration of nvcuda::wmma::accumulator type is not supported.
  144 |     wmma::fragment<wmma::accumulator, WMMA_M, WMMA_N, WMMA_K, half> QKV_mat;
      |                          ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused.cu:145:5: warning: DPCT1007:237: Migration of nvcuda::wmma::fill_fragment is not supported.
  145 |     wmma::fill_fragment(QKV_mat, 0.0f);
      |     ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused.cu:154:7: warning: DPCT1007:238: Migration of nvcuda::wmma::mma_sync is not supported.
  154 |       wmma::mma_sync(QKV_mat, Logits_mat, V_mat, QKV_mat);
      |       ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused.cu:83:5: warning: DPCT1007:239: Migration of nvcuda::wmma::store_matrix_sync is not supported.
   83 |     wmma::store_matrix_sync(s_logits[warp_from_offset] + warp_to_offset, QK_mat,
      |     ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused.cu:78:7: warning: DPCT1007:240: Migration of nvcuda::wmma::load_matrix_sync is not supported.
   78 |       wmma::load_matrix_sync(Q_mat, s_query[warp_from_offset] + k * WMMA_K,
      |       ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused.cu:80:7: warning: DPCT1007:241: Migration of nvcuda::wmma::load_matrix_sync is not supported.
   80 |       wmma::load_matrix_sync(K_mat, s_kv[warp_to_offset] + k * WMMA_K, size_per_head + SKEW_HALF);
      |       ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused.cu:156:5: warning: DPCT1007:242: Migration of nvcuda::wmma::store_matrix_sync is not supported.
  156 |     wmma::store_matrix_sync(s_query[warp_from_offset] + warp_to_offset, QKV_mat,
      |     ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused.cu:151:7: warning: DPCT1007:243: Migration of nvcuda::wmma::load_matrix_sync is not supported.
  151 |       wmma::load_matrix_sync(Logits_mat, s_logits[warp_from_offset] + k * WMMA_K,
      |       ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused.cu:153:7: warning: DPCT1007:244: Migration of nvcuda::wmma::load_matrix_sync is not supported.
  153 |       wmma::load_matrix_sync(V_mat, s_kv[k * WMMA_K] + warp_to_offset, size_per_head + SKEW_HALF);
      |       ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused.cu:169:1: warning: DPCT1110:245: The total declared local variable size in device function wmma_attention_kernel exceeds 128 bytes and may cause high register pressure. Consult with your hardware vendor to find the total register size available and adjust the code, or use smaller sub-group size to avoid high register pressure.
  169 | __global__
      | ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused.cu:201:11: warning: DPCT1082:246: Migration of nvcuda::wmma::fragment<wmma::matrix_a, 16, 16, 16, half, wmma::row_major> type is not supported.
  201 |     wmma::fragment<wmma::matrix_a, WMMA_M, WMMA_N, WMMA_K, half, wmma::row_major> Q_mat;
      |           ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused.cu:201:26: warning: DPCT1082:247: Migration of nvcuda::wmma::matrix_a type is not supported.
  201 |     wmma::fragment<wmma::matrix_a, WMMA_M, WMMA_N, WMMA_K, half, wmma::row_major> Q_mat;
      |                          ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused.cu:201:72: warning: DPCT1082:248: Migration of nvcuda::wmma::row_major type is not supported.
  201 |     wmma::fragment<wmma::matrix_a, WMMA_M, WMMA_N, WMMA_K, half, wmma::row_major> Q_mat;
      |                                                                        ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused.cu:202:11: warning: DPCT1082:249: Migration of nvcuda::wmma::fragment<wmma::matrix_b, 16, 16, 16, half, wmma::col_major> type is not supported.
  202 |     wmma::fragment<wmma::matrix_b, WMMA_M, WMMA_N, WMMA_K, half, wmma::col_major> K_mat;
      |           ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused.cu:202:26: warning: DPCT1082:250: Migration of nvcuda::wmma::matrix_b type is not supported.
  202 |     wmma::fragment<wmma::matrix_b, WMMA_M, WMMA_N, WMMA_K, half, wmma::col_major> K_mat;
      |                          ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused.cu:202:72: warning: DPCT1082:251: Migration of nvcuda::wmma::col_major type is not supported.
  202 |     wmma::fragment<wmma::matrix_b, WMMA_M, WMMA_N, WMMA_K, half, wmma::col_major> K_mat;
      |                                                                        ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused.cu:203:11: warning: DPCT1082:252: Migration of nvcuda::wmma::fragment<wmma::accumulator, 16, 16, 16, half> type is not supported.
  203 |     wmma::fragment<wmma::accumulator, WMMA_M, WMMA_N, WMMA_K, half> QK_mat;
      |           ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused.cu:203:26: warning: DPCT1082:253: Migration of nvcuda::wmma::accumulator type is not supported.
  203 |     wmma::fragment<wmma::accumulator, WMMA_M, WMMA_N, WMMA_K, half> QK_mat;
      |                          ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused.cu:204:5: warning: DPCT1007:254: Migration of nvcuda::wmma::fill_fragment is not supported.
  204 |     wmma::fill_fragment(QK_mat, 0.0f);
      |     ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused.cu:213:7: warning: DPCT1007:255: Migration of nvcuda::wmma::mma_sync is not supported.
  213 |       wmma::mma_sync(QK_mat, Q_mat, K_mat, QK_mat);
      |       ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused.cu:269:11: warning: DPCT1082:256: Migration of nvcuda::wmma::fragment<wmma::matrix_a, 16, 16, 16, half, wmma::row_major> type is not supported.
  269 |     wmma::fragment<wmma::matrix_a, WMMA_M, WMMA_N, WMMA_K, half, wmma::row_major> Logits_mat;
      |           ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused.cu:269:26: warning: DPCT1082:257: Migration of nvcuda::wmma::matrix_a type is not supported.
  269 |     wmma::fragment<wmma::matrix_a, WMMA_M, WMMA_N, WMMA_K, half, wmma::row_major> Logits_mat;
      |                          ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused.cu:269:72: warning: DPCT1082:258: Migration of nvcuda::wmma::row_major type is not supported.
  269 |     wmma::fragment<wmma::matrix_a, WMMA_M, WMMA_N, WMMA_K, half, wmma::row_major> Logits_mat;
      |                                                                        ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused.cu:270:11: warning: DPCT1082:259: Migration of nvcuda::wmma::fragment<wmma::matrix_b, 16, 16, 16, half, wmma::row_major> type is not supported.
  270 |     wmma::fragment<wmma::matrix_b, WMMA_M, WMMA_N, WMMA_K, half, wmma::row_major> V_mat;
      |           ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused.cu:270:26: warning: DPCT1082:260: Migration of nvcuda::wmma::matrix_b type is not supported.
  270 |     wmma::fragment<wmma::matrix_b, WMMA_M, WMMA_N, WMMA_K, half, wmma::row_major> V_mat;
      |                          ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused.cu:270:72: warning: DPCT1082:261: Migration of nvcuda::wmma::row_major type is not supported.
  270 |     wmma::fragment<wmma::matrix_b, WMMA_M, WMMA_N, WMMA_K, half, wmma::row_major> V_mat;
      |                                                                        ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused.cu:271:11: warning: DPCT1082:262: Migration of nvcuda::wmma::fragment<wmma::accumulator, 16, 16, 16, half> type is not supported.
  271 |     wmma::fragment<wmma::accumulator, WMMA_M, WMMA_N, WMMA_K, half> QKV_mat;
      |           ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused.cu:271:26: warning: DPCT1082:263: Migration of nvcuda::wmma::accumulator type is not supported.
  271 |     wmma::fragment<wmma::accumulator, WMMA_M, WMMA_N, WMMA_K, half> QKV_mat;
      |                          ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused.cu:272:5: warning: DPCT1007:264: Migration of nvcuda::wmma::fill_fragment is not supported.
  272 |     wmma::fill_fragment(QKV_mat, 0.0f);
      |     ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused.cu:281:7: warning: DPCT1007:265: Migration of nvcuda::wmma::mma_sync is not supported.
  281 |       wmma::mma_sync(QKV_mat, Logits_mat, V_mat, QKV_mat);
      |       ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused.cu:215:5: warning: DPCT1007:266: Migration of nvcuda::wmma::store_matrix_sync is not supported.
  215 |     wmma::store_matrix_sync(s_logits[warp_from_offset] + warp_to_offset, QK_mat,
      |     ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused.cu:210:7: warning: DPCT1007:267: Migration of nvcuda::wmma::load_matrix_sync is not supported.
  210 |       wmma::load_matrix_sync(Q_mat, s_query[warp_from_offset] + k * WMMA_K,
      |       ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused.cu:212:7: warning: DPCT1007:268: Migration of nvcuda::wmma::load_matrix_sync is not supported.
  212 |       wmma::load_matrix_sync(K_mat, s_kv[warp_to_offset] + k * WMMA_K, size_per_head + SKEW_HALF);
      |       ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused.cu:283:5: warning: DPCT1007:269: Migration of nvcuda::wmma::store_matrix_sync is not supported.
  283 |     wmma::store_matrix_sync(s_query[warp_from_offset] + warp_to_offset, QKV_mat,
      |     ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused.cu:278:7: warning: DPCT1007:270: Migration of nvcuda::wmma::load_matrix_sync is not supported.
  278 |       wmma::load_matrix_sync(Logits_mat, s_logits[warp_from_offset] + k * WMMA_K,
      |       ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused.cu:280:7: warning: DPCT1007:271: Migration of nvcuda::wmma::load_matrix_sync is not supported.
  280 |       wmma::load_matrix_sync(V_mat, s_kv[k * WMMA_K] + warp_to_offset, size_per_head + SKEW_HALF);
      |       ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused.cu:296:1: warning: DPCT1110:272: The total declared local variable size in device function wmma_attention_rm_kernel exceeds 128 bytes and may cause high register pressure. Consult with your hardware vendor to find the total register size available and adjust the code, or use smaller sub-group size to avoid high register pressure.
  296 | __global__
      | ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused.cu:329:11: warning: DPCT1082:273: Migration of nvcuda::wmma::fragment<wmma::matrix_a, 16, 16, 16, half, wmma::row_major> type is not supported.
  329 |     wmma::fragment<wmma::matrix_a, WMMA_M, WMMA_N, WMMA_K, half, wmma::row_major> Q_mat;
      |           ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused.cu:329:26: warning: DPCT1082:274: Migration of nvcuda::wmma::matrix_a type is not supported.
  329 |     wmma::fragment<wmma::matrix_a, WMMA_M, WMMA_N, WMMA_K, half, wmma::row_major> Q_mat;
      |                          ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused.cu:329:72: warning: DPCT1082:275: Migration of nvcuda::wmma::row_major type is not supported.
  329 |     wmma::fragment<wmma::matrix_a, WMMA_M, WMMA_N, WMMA_K, half, wmma::row_major> Q_mat;
      |                                                                        ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused.cu:330:11: warning: DPCT1082:276: Migration of nvcuda::wmma::fragment<wmma::matrix_b, 16, 16, 16, half, wmma::col_major> type is not supported.
  330 |     wmma::fragment<wmma::matrix_b, WMMA_M, WMMA_N, WMMA_K, half, wmma::col_major> K_mat;
      |           ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused.cu:330:26: warning: DPCT1082:277: Migration of nvcuda::wmma::matrix_b type is not supported.
  330 |     wmma::fragment<wmma::matrix_b, WMMA_M, WMMA_N, WMMA_K, half, wmma::col_major> K_mat;
      |                          ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused.cu:330:72: warning: DPCT1082:278: Migration of nvcuda::wmma::col_major type is not supported.
  330 |     wmma::fragment<wmma::matrix_b, WMMA_M, WMMA_N, WMMA_K, half, wmma::col_major> K_mat;
      |                                                                        ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused.cu:331:11: warning: DPCT1082:279: Migration of nvcuda::wmma::fragment<wmma::accumulator, 16, 16, 16, half> type is not supported.
  331 |     wmma::fragment<wmma::accumulator, WMMA_M, WMMA_N, WMMA_K, half> QK_mat;
      |           ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused.cu:331:26: warning: DPCT1082:280: Migration of nvcuda::wmma::accumulator type is not supported.
  331 |     wmma::fragment<wmma::accumulator, WMMA_M, WMMA_N, WMMA_K, half> QK_mat;
      |                          ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused.cu:332:5: warning: DPCT1007:281: Migration of nvcuda::wmma::fill_fragment is not supported.
  332 |     wmma::fill_fragment(QK_mat, 0.0f);
      |     ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused.cu:341:7: warning: DPCT1007:282: Migration of nvcuda::wmma::mma_sync is not supported.
  341 |       wmma::mma_sync(QK_mat, Q_mat, K_mat, QK_mat);
      |       ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused.cu:390:11: warning: DPCT1082:283: Migration of nvcuda::wmma::fragment<wmma::matrix_a, 16, 16, 16, half, wmma::row_major> type is not supported.
  390 |     wmma::fragment<wmma::matrix_a, WMMA_M, WMMA_N, WMMA_K, half, wmma::row_major> Logits_mat;
      |           ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused.cu:390:26: warning: DPCT1082:284: Migration of nvcuda::wmma::matrix_a type is not supported.
  390 |     wmma::fragment<wmma::matrix_a, WMMA_M, WMMA_N, WMMA_K, half, wmma::row_major> Logits_mat;
      |                          ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused.cu:390:72: warning: DPCT1082:285: Migration of nvcuda::wmma::row_major type is not supported.
  390 |     wmma::fragment<wmma::matrix_a, WMMA_M, WMMA_N, WMMA_K, half, wmma::row_major> Logits_mat;
      |                                                                        ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused.cu:391:11: warning: DPCT1082:286: Migration of nvcuda::wmma::fragment<wmma::matrix_b, 16, 16, 16, half, wmma::row_major> type is not supported.
  391 |     wmma::fragment<wmma::matrix_b, WMMA_M, WMMA_N, WMMA_K, half, wmma::row_major> V_mat;
      |           ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused.cu:391:26: warning: DPCT1082:287: Migration of nvcuda::wmma::matrix_b type is not supported.
  391 |     wmma::fragment<wmma::matrix_b, WMMA_M, WMMA_N, WMMA_K, half, wmma::row_major> V_mat;
      |                          ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused.cu:391:72: warning: DPCT1082:288: Migration of nvcuda::wmma::row_major type is not supported.
  391 |     wmma::fragment<wmma::matrix_b, WMMA_M, WMMA_N, WMMA_K, half, wmma::row_major> V_mat;
      |                                                                        ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused.cu:392:11: warning: DPCT1082:289: Migration of nvcuda::wmma::fragment<wmma::accumulator, 16, 16, 16, half> type is not supported.
  392 |     wmma::fragment<wmma::accumulator, WMMA_M, WMMA_N, WMMA_K, half> QKV_mat;
      |           ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused.cu:392:26: warning: DPCT1082:290: Migration of nvcuda::wmma::accumulator type is not supported.
  392 |     wmma::fragment<wmma::accumulator, WMMA_M, WMMA_N, WMMA_K, half> QKV_mat;
      |                          ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused.cu:393:5: warning: DPCT1007:291: Migration of nvcuda::wmma::fill_fragment is not supported.
  393 |     wmma::fill_fragment(QKV_mat, 0.0f);
      |     ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused.cu:402:7: warning: DPCT1007:292: Migration of nvcuda::wmma::mma_sync is not supported.
  402 |       wmma::mma_sync(QKV_mat, Logits_mat, V_mat, QKV_mat);
      |       ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused.cu:343:5: warning: DPCT1007:293: Migration of nvcuda::wmma::store_matrix_sync is not supported.
  343 |     wmma::store_matrix_sync(s_logits[warp_from_offset] + warp_to_offset, QK_mat,
      |     ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused.cu:338:7: warning: DPCT1007:294: Migration of nvcuda::wmma::load_matrix_sync is not supported.
  338 |       wmma::load_matrix_sync(Q_mat, s_query[warp_from_offset] + k * WMMA_K,
      |       ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused.cu:340:7: warning: DPCT1007:295: Migration of nvcuda::wmma::load_matrix_sync is not supported.
  340 |       wmma::load_matrix_sync(K_mat, s_kv[warp_to_offset] + k * WMMA_K, size_per_head + SKEW_HALF);
      |       ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused.cu:404:5: warning: DPCT1007:296: Migration of nvcuda::wmma::store_matrix_sync is not supported.
  404 |     wmma::store_matrix_sync(s_query[warp_from_offset] + warp_to_offset, QKV_mat,
      |     ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused.cu:399:7: warning: DPCT1007:297: Migration of nvcuda::wmma::load_matrix_sync is not supported.
  399 |       wmma::load_matrix_sync(Logits_mat, s_logits[warp_from_offset] + k * WMMA_K,
      |       ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused.cu:401:7: warning: DPCT1007:298: Migration of nvcuda::wmma::load_matrix_sync is not supported.
  401 |       wmma::load_matrix_sync(V_mat, s_kv[k * WMMA_K] + warp_to_offset, size_per_head + SKEW_HALF);
      |       ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused.cu:421:3: warning: DPCT1049:299: The work-group size passed to the SYCL kernel may exceed the limit. To get the device limit, query info::device::max_work_group_size. Adjust the work-group size if needed.
  421 |   wmma_attention_kernel<SEQ_LEN, SIZE_PER_HEAD><<<grid, block, 0, infer_param.stream>>>( \
      |   ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused.cu:417:3: warning: DPCT1049:300: The work-group size passed to the SYCL kernel may exceed the limit. To get the device limit, query info::device::max_work_group_size. Adjust the work-group size if needed.
  417 |   wmma_attention_kernel_16<SEQ_LEN, SIZE_PER_HEAD><<<grid, block, 0, infer_param.stream>>>( \
      |   ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused.cu:425:3: warning: DPCT1049:301: The work-group size passed to the SYCL kernel may exceed the limit. To get the device limit, query info::device::max_work_group_size. Adjust the work-group size if needed.
  425 |   wmma_attention_rm_kernel<SEQ_LEN, SIZE_PER_HEAD><<<grid, block, 0, infer_param.stream>>>( \
      |   ^
In file included from /export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/bert_transformer.cu:24:
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/remove_padding.h:29:13: warning: DPCT1121:302: Make sure that the "count" which is used in the SYCL group function/algorithm is initialized.
   29 |     T val = __shfl_up_sync(0xffffffff, count, i);
      |             ^
In file included from /export/users/zhaojiam/src/ByteTransformer/bytetransformer/th_op/ths_op_f.cc:21:
In file included from /export/users/zhaojiam/src/ByteTransformer/bytetransformer/th_op/bert_transformer_ext.h:21:
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/common.h:92:10: warning: DPCT1009:303: SYCL uses exceptions to report errors and does not use the error codes. The original code was commented out and a warning string was inserted. You need to rewrite this code.
   92 |   return cudaGetErrorString(error);
      |          ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused_long.cu:318:9: warning: DPCT1026:304: The call to cudaFuncSetAttribute was removed because SYCL currently does not support corresponding setting.
  318 | #define WMMA_ATTENTION_LONG(SEQ_LEN, SIZE_PER_HEAD, SPLIT_LEN)                                    \
      |         ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused_long.cu:332:9: warning: DPCT1026:305: The call to cudaFuncSetAttribute was removed because SYCL currently does not support corresponding setting.
  332 | #define WMMA_ATTENTION_LONG_RM(SEQ_LEN, SIZE_PER_HEAD, SPLIT_LEN)                                 \
      |         ^
In file included from /export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused_long.cu:21:
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/reduce.h:49:3: warning: DPCT1065:306: Consider replacing sycl::nd_item::barrier() with sycl::nd_item::barrier(sycl::access::fence_space::local_space) for better performance if there is no access to global memory.
   49 |   __syncthreads();
      |   ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/reduce.h:76:3: warning: DPCT1065:307: Consider replacing sycl::nd_item::barrier() with sycl::nd_item::barrier(sycl::access::fence_space::local_space) for better performance if there is no access to global memory.
   76 |   __syncthreads();
      |   ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/reduce.h:106:3: warning: DPCT1065:308: Consider replacing sycl::nd_item::barrier() with sycl::nd_item::barrier(sycl::access::fence_space::local_space) for better performance if there is no access to global memory.
  106 |   __syncthreads();
      |   ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused_long.cu:57:18: warning: DPCT1098:309: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
   57 |   half2 q_bias = __ldg(&qkv_bias[thread_offset]);
      |                  ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused_long.cu:61:47: warning: DPCT1098:310: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
   61 |     *(__half2 *)(*s_query + offset) = __hadd2(__ldg(&qkv[pos]), q_bias);
      |                                               ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused_long.cu:65:18: warning: DPCT1098:311: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
   65 |   half2 k_bias = __ldg(&qkv_bias[thread_offset + half_hidden_dim]);
      |                  ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused_long.cu:69:44: warning: DPCT1098:312: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
   69 |     *(__half2 *)(*s_kv + offset) = __hadd2(__ldg(&qkv[pos + half_hidden_dim]), k_bias);
      |                                            ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused_long.cu:131:18: warning: DPCT1098:313: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
  131 |   half2 v_bias = __ldg(&qkv_bias[thread_offset + half_hidden_dim * 2]);
      |                  ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused_long.cu:135:17: warning: DPCT1098:314: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
  135 |         __hadd2(__ldg(&qkv[pos + half_hidden_dim * 2]), v_bias);
      |                 ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused_long.cu:71:3: warning: DPCT1065:315: Consider replacing sycl::nd_item::barrier() with sycl::nd_item::barrier(sycl::access::fence_space::local_space) for better performance if there is no access to global memory.
   71 |   __syncthreads();
      |   ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused_long.cu:91:3: warning: DPCT1065:316: Consider replacing sycl::nd_item::barrier() with sycl::nd_item::barrier(sycl::access::fence_space::local_space) for better performance if there is no access to global memory.
   91 |   __syncthreads();
      |   ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused_long.cu:141:3: warning: DPCT1065:317: Consider replacing sycl::nd_item::barrier() with sycl::nd_item::barrier(sycl::access::fence_space::local_space) for better performance if there is no access to global memory.
  141 |   __syncthreads();
      |   ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused_long.cu:162:3: warning: DPCT1065:318: Consider replacing sycl::nd_item::barrier() with sycl::nd_item::barrier(sycl::access::fence_space::local_space) for better performance if there is no access to global memory.
  162 |   __syncthreads();
      |   ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused_long.cu:108:20: warning: DPCT1098:319: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
  108 |             (float)__ldg(&attention_mask[(batch_seq_block_offset + from_id) * seq_len + to_id[i]]);
      |                    ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused_long.cu:192:32: warning: DPCT1098:320: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
  192 |   const int batch_seq_offset = __ldg(&batch_idx[blockIdx.z]);
      |                                ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused_long.cu:193:29: warning: DPCT1098:321: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
  193 |   const int batch_seq_len = __ldg(&batch_idx[blockIdx.z + 1]) - batch_seq_offset;
      |                             ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused_long.cu:204:18: warning: DPCT1098:322: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
  204 |   half2 q_bias = __ldg(&qkv_bias[thread_offset]);
      |                  ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused_long.cu:208:47: warning: DPCT1098:323: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
  208 |     *(__half2 *)(*s_query + offset) = __hadd2(__ldg(&qkv[pos]), q_bias);
      |                                               ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused_long.cu:212:18: warning: DPCT1098:324: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
  212 |   half2 k_bias = __ldg(&qkv_bias[thread_offset + half_hidden_dim]);
      |                  ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused_long.cu:216:44: warning: DPCT1098:325: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
  216 |     *(__half2 *)(*s_kv + offset) = __hadd2(__ldg(&qkv[pos + half_hidden_dim]), k_bias);
      |                                            ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused_long.cu:274:18: warning: DPCT1098:326: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
  274 |   half2 v_bias = __ldg(&qkv_bias[thread_offset + half_hidden_dim * 2]);
      |                  ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused_long.cu:278:17: warning: DPCT1098:327: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
  278 |         __hadd2(__ldg(&qkv[pos + half_hidden_dim * 2]), v_bias);
      |                 ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused_long.cu:218:3: warning: DPCT1065:328: Consider replacing sycl::nd_item::barrier() with sycl::nd_item::barrier(sycl::access::fence_space::local_space) for better performance if there is no access to global memory.
  218 |   __syncthreads();
      |   ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused_long.cu:241:3: warning: DPCT1065:329: Consider replacing sycl::nd_item::barrier() with sycl::nd_item::barrier(sycl::access::fence_space::local_space) for better performance if there is no access to global memory.
  241 |   __syncthreads();
      |   ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused_long.cu:284:3: warning: DPCT1065:330: Consider replacing sycl::nd_item::barrier() with sycl::nd_item::barrier(sycl::access::fence_space::local_space) for better performance if there is no access to global memory.
  284 |   __syncthreads();
      |   ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused_long.cu:305:3: warning: DPCT1065:331: Consider replacing sycl::nd_item::barrier() with sycl::nd_item::barrier(sycl::access::fence_space::local_space) for better performance if there is no access to global memory.
  305 |   __syncthreads();
      |   ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/gemm_bias_act.cu:27:44: warning: DPCT1098:332: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
   27 |     float out = output[row_offset + tid] + __ldg(&bias[tid]);
      |                                            ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/gemm_bias_act.cu:41:57: warning: DPCT1098:333: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
   41 |       half2 out = __hadd2(output_ptr[row_offset + tid], __ldg(&bias_ptr[tid]));
      |                                                         ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/gemm_bias_act.cu:51:20: warning: DPCT1098:334: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
   51 |       bias_val.x = __ldg(&bias_ptr[tid]);
      |                    ^
In file included from /export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_nofused.cu:23:
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/softmax.h:111:17: warning: DPCT1098:335: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
  111 |   int word_id = __ldg(&word_idx[blockIdx.x]);
      |                 ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/softmax.h:111:17: warning: DPCT1064:336: Migrated __ldg call is used in a macro/template definition and may not be valid for all macro/template uses. Adjust the code.
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/softmax.h:152:17: warning: DPCT1098:337: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
  152 |   int word_id = __ldg(&word_idx[blockIdx.x]);
      |                 ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/softmax.h:152:17: warning: DPCT1064:338: Migrated __ldg call is used in a macro/template definition and may not be valid for all macro/template uses. Adjust the code.
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/softmax.h:216:23: warning: DPCT1098:339: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
  216 |           __hadd2(qk, __ldg(&atten_bias[((head_id * seq_len + seq_id) * half2_seq_len) + col_id]));
      |                       ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/softmax.h:217:22: warning: DPCT1098:340: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
  217 |     half2 mask_val = __ldg(&atten_mask[((batch_id * seq_len + seq_id) * half2_seq_len) + col_id]);
      |                      ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/softmax.h:250:17: warning: DPCT1098:341: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
  250 |   int word_id = __ldg(&word_idx[blockIdx.x]);
      |                 ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/softmax.h:271:23: warning: DPCT1098:342: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
  271 |           __hadd2(qk, __ldg(&atten_bias[((head_id * seq_len + seq_id) * half2_seq_len) + col_id]));
      |                       ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/softmax.h:272:22: warning: DPCT1098:343: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
  272 |     half2 mask_val = __ldg(&atten_mask[((batch_id * seq_len + seq_id) * half2_seq_len) + col_id]);
      |                      ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/softmax.h:250:17: warning: DPCT1064:344: Migrated __ldg call is used in a macro/template definition and may not be valid for all macro/template uses. Adjust the code.
  250 |   int word_id = __ldg(&word_idx[blockIdx.x]);
      |                 ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/softmax.h:382:9: warning: DPCT1026:345: The call to cudaFuncSetAttribute was removed because SYCL currently does not support corresponding setting.
  382 |         cudaFuncSetAttribute(softmax_kernel_warp_half2<half2>,
      |         ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/softmax.h:390:7: warning: DPCT1026:346: The call to cudaFuncSetAttribute was removed because SYCL currently does not support corresponding setting.
  390 |       cudaFuncSetAttribute(softmax_kernel_warp<T>, cudaFuncAttributeMaxDynamicSharedMemorySize,
      |       ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/softmax.h:462:9: warning: DPCT1026:347: The call to cudaFuncSetAttribute was removed because SYCL currently does not support corresponding setting.
  462 |         cudaFuncSetAttribute(softmax_kernel_warp_half2_et<half2>,
      |         ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/softmax.h:470:7: warning: DPCT1026:348: The call to cudaFuncSetAttribute was removed because SYCL currently does not support corresponding setting.
  470 |       cudaFuncSetAttribute(softmax_kernel_warp_et<T>, cudaFuncAttributeMaxDynamicSharedMemorySize,
      |       ^
In file included from /export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_nofused.cu:24:
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:59:39: warning: DPCT1098:349: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
   59 |     *(__half2 *)(*s_query + offset) = __ldg(&query[pos]);
      |                                       ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:60:36: warning: DPCT1098:350: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
   60 |     *(__half2 *)(*s_kv + offset) = __ldg(&key[pos]);
      |                                    ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:127:46: warning: DPCT1098:351: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
  127 |     ((__half2 *)(s_kv[from_id]))[warp_tid] = __ldg(&value[pos]);
      |                                              ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:62:3: warning: DPCT1065:352: Consider replacing sycl::nd_item::barrier() with sycl::nd_item::barrier(sycl::access::fence_space::local_space) for better performance if there is no access to global memory.
   62 |   __syncthreads();
      |   ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:82:3: warning: DPCT1065:353: Consider replacing sycl::nd_item::barrier() with sycl::nd_item::barrier(sycl::access::fence_space::local_space) for better performance if there is no access to global memory.
   82 |   __syncthreads();
      |   ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:133:3: warning: DPCT1065:354: Consider replacing sycl::nd_item::barrier() with sycl::nd_item::barrier(sycl::access::fence_space::local_space) for better performance if there is no access to global memory.
  133 |   __syncthreads();
      |   ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:154:3: warning: DPCT1065:355: Consider replacing sycl::nd_item::barrier() with sycl::nd_item::barrier(sycl::access::fence_space::local_space) for better performance if there is no access to global memory.
  154 |   __syncthreads();
      |   ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:104:20: warning: DPCT1098:356: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
  104 |             (float)__ldg(&attention_mask[(batch_seq_offset + from_id) * seq_len + to_id[i]]);
      |                    ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:182:28: warning: DPCT1098:357: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
  182 |   const int batch_offset = __ldg(&batch_idx[blockIdx.y]);
      |                            ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:183:29: warning: DPCT1098:358: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
  183 |   const int batch_seq_len = __ldg(&batch_idx[blockIdx.y + 1]) - batch_offset;
      |                             ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:193:39: warning: DPCT1098:359: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
  193 |     *(__half2 *)(*s_query + offset) = __ldg(&query[pos]);
      |                                       ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:194:36: warning: DPCT1098:360: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
  194 |     *(__half2 *)(*s_kv + offset) = __ldg(&key[pos]);
      |                                    ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:258:46: warning: DPCT1098:361: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
  258 |     ((__half2 *)(s_kv[from_id]))[warp_tid] = __ldg(&value[pos]);
      |                                              ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:196:3: warning: DPCT1065:362: Consider replacing sycl::nd_item::barrier() with sycl::nd_item::barrier(sycl::access::fence_space::local_space) for better performance if there is no access to global memory.
  196 |   __syncthreads();
      |   ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:216:3: warning: DPCT1065:363: Consider replacing sycl::nd_item::barrier() with sycl::nd_item::barrier(sycl::access::fence_space::local_space) for better performance if there is no access to global memory.
  216 |   __syncthreads();
      |   ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:264:3: warning: DPCT1065:364: Consider replacing sycl::nd_item::barrier() with sycl::nd_item::barrier(sycl::access::fence_space::local_space) for better performance if there is no access to global memory.
  264 |   __syncthreads();
      |   ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:285:3: warning: DPCT1065:365: Consider replacing sycl::nd_item::barrier() with sycl::nd_item::barrier(sycl::access::fence_space::local_space) for better performance if there is no access to global memory.
  285 |   __syncthreads();
      |   ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:327:39: warning: DPCT1098:366: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
  327 |     *(__half2 *)(*s_query + offset) = __ldg(&query[pos]);
      |                                       ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:334:36: warning: DPCT1098:367: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
  334 |     *(__half2 *)(*s_kv + offset) = __ldg(&key[pos]);
      |                                    ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:407:45: warning: DPCT1098:368: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
  407 |     ((__half2 *)(s_kv[seq_id]))[warp_tid] = __ldg(&value[pos]);
      |                                             ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:336:3: warning: DPCT1065:369: Consider replacing sycl::nd_item::barrier() with sycl::nd_item::barrier(sycl::access::fence_space::local_space) for better performance if there is no access to global memory.
  336 |   __syncthreads();
      |   ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:356:3: warning: DPCT1065:370: Consider replacing sycl::nd_item::barrier() with sycl::nd_item::barrier(sycl::access::fence_space::local_space) for better performance if there is no access to global memory.
  356 |   __syncthreads();
      |   ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:413:3: warning: DPCT1065:371: Consider replacing sycl::nd_item::barrier() with sycl::nd_item::barrier(sycl::access::fence_space::local_space) for better performance if there is no access to global memory.
  413 |   __syncthreads();
      |   ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:434:3: warning: DPCT1065:372: Consider replacing sycl::nd_item::barrier() with sycl::nd_item::barrier(sycl::access::fence_space::local_space) for better performance if there is no access to global memory.
  434 |   __syncthreads();
      |   ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:381:29: warning: DPCT1098:373: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
  381 |         float mask = (float)__ldg(
      |                             ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:464:32: warning: DPCT1098:374: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
  464 |   const int batch_seq_offset = __ldg(&batch_idx[blockIdx.z]);
      |                                ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:465:29: warning: DPCT1098:375: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
  465 |   const int batch_seq_len = __ldg(&batch_idx[blockIdx.z + 1]) - batch_seq_offset;
      |                             ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:481:39: warning: DPCT1098:376: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
  481 |     *(__half2 *)(*s_query + offset) = __ldg(&query[pos]);
      |                                       ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:488:36: warning: DPCT1098:377: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
  488 |     *(__half2 *)(*s_kv + offset) = __ldg(&key[pos]);
      |                                    ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:561:45: warning: DPCT1098:378: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
  561 |     ((__half2 *)(s_kv[seq_id]))[warp_tid] = __ldg(&value[pos]);
      |                                             ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:490:3: warning: DPCT1065:379: Consider replacing sycl::nd_item::barrier() with sycl::nd_item::barrier(sycl::access::fence_space::local_space) for better performance if there is no access to global memory.
  490 |   __syncthreads();
      |   ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:513:3: warning: DPCT1065:380: Consider replacing sycl::nd_item::barrier() with sycl::nd_item::barrier(sycl::access::fence_space::local_space) for better performance if there is no access to global memory.
  513 |   __syncthreads();
      |   ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:567:3: warning: DPCT1065:381: Consider replacing sycl::nd_item::barrier() with sycl::nd_item::barrier(sycl::access::fence_space::local_space) for better performance if there is no access to global memory.
  567 |   __syncthreads();
      |   ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:588:3: warning: DPCT1065:382: Consider replacing sycl::nd_item::barrier() with sycl::nd_item::barrier(sycl::access::fence_space::local_space) for better performance if there is no access to global memory.
  588 |   __syncthreads();
      |   ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:606:9: warning: DPCT1026:383: The call to cudaFuncSetAttribute was removed because SYCL currently does not support corresponding setting.
  606 | #define X_WMMA_ATTENTION_LONG(SEQ_LEN, SIZE_PER_HEAD, SPLIT_LEN)                                  \
      |         ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/variety_attention_fused.h:620:9: warning: DPCT1026:384: The call to cudaFuncSetAttribute was removed because SYCL currently does not support corresponding setting.
  620 | #define X_WMMA_ATTENTION_LONG_RM(SEQ_LEN, SIZE_PER_HEAD, SPLIT_LEN)                               \
      |         ^
In file included from /export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/cutlass_attention.cu:24:
In file included from /export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/cutlass_attention_defs.h:33:
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/cutlass_attention_operators.h:56:53: warning: DPCT1098:385: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
   56 |                                                   : __ldg(&seqlen_offsets_[problem_idx + 1]) -
      |                                                     ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/cutlass_attention_operators.h:57:57: warning: DPCT1098:386: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
   57 |                                                         __ldg(&seqlen_offsets_[problem_idx]);
      |                                                         ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/cutlass_attention_operators.h:131:26: warning: DPCT1098:387: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
  131 |       int batch_offset = __ldg(&seqlen_offsets_[problem_idx]);
      |                          ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/cutlass_attention_operators.h:387:14: warning: DPCT1098:388: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
  387 |           ? (__ldg(&seqlen_offsets[batch_idx + 1]) - __ldg(&seqlen_offsets[batch_idx]))
      |              ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/cutlass_attention_operators.h:397:15: warning: DPCT1098:389: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
  397 |     max_val = __ldg(row_buf);
      |               ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/cutlass_attention_operators.h:399:30: warning: DPCT1098:390: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
  399 |       max_val = max(max_val, __ldg(row_buf + i * 2));
      |                              ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/cutlass_attention_operators.h:403:18: warning: DPCT1098:391: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
  403 |       sum_val += __ldg(row_buf + i * 2 + 1) * expf(__ldg(row_buf + i * 2) - max_val);
      |                  ^
In file included from /export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/layernorm.cu:20:
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/layernorm.h:30:3: warning: DPCT1065:392: Consider replacing sycl::nd_item::barrier() with sycl::nd_item::barrier(sycl::access::fence_space::local_space) for better performance if there is no access to global memory.
   30 |   __syncthreads();
      |   ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/layernorm.h:36:3: warning: DPCT1065:393: Consider replacing sycl::nd_item::barrier() with sycl::nd_item::barrier(sycl::access::fence_space::local_space) for better performance if there is no access to global memory.
   36 |   __syncthreads();
      |   ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/layernorm.h:39:14: warning: DPCT1098:394: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
   39 |              __ldg(&((float *)beta)[threadIdx.x]);
      |              ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/layernorm.h:38:34: warning: DPCT1098:395: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
   38 |   *out_ptr = local_out * s_[1] * __ldg(&((float *)gamma)[threadIdx.x]) +
      |                                  ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/layernorm.h:49:3: warning: DPCT1065:396: Consider replacing sycl::nd_item::barrier() with sycl::nd_item::barrier(sycl::access::fence_space::local_space) for better performance if there is no access to global memory.
   49 |   __syncthreads();
      |   ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/layernorm.h:59:3: warning: DPCT1065:397: Consider replacing sycl::nd_item::barrier() with sycl::nd_item::barrier(sycl::access::fence_space::local_space) for better performance if there is no access to global memory.
   59 |   __syncthreads();
      |   ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/layernorm.h:64:19: warning: DPCT1098:398: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
   64 |       gamma_val = __ldg(&((const float2 *)gamma)[threadIdx.x]);
      |                   ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/layernorm.h:65:18: warning: DPCT1098:399: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
   65 |       beta_val = __ldg(&((const float2 *)beta)[threadIdx.x]);
      |                  ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/layernorm.h:67:34: warning: DPCT1098:400: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
   67 |       gamma_val = __half22float2(__ldg(&((const half2 *)gamma)[threadIdx.x]));
      |                                  ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/layernorm.h:67:34: warning: DPCT1064:401: Migrated __ldg call is used in a macro/template definition and may not be valid for all macro/template uses. Adjust the code.
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/layernorm.h:68:33: warning: DPCT1098:402: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
   68 |       beta_val = __half22float2(__ldg(&((const half2 *)beta)[threadIdx.x]));
      |                                 ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/layernorm.h:68:33: warning: DPCT1064:403: Migrated __ldg call is used in a macro/template definition and may not be valid for all macro/template uses. Adjust the code.
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/layernorm.h:92:3: warning: DPCT1065:404: Consider replacing sycl::nd_item::barrier() with sycl::nd_item::barrier(sycl::access::fence_space::local_space) for better performance if there is no access to global memory.
   92 |   __syncthreads();
      |   ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/layernorm.h:104:3: warning: DPCT1065:405: Consider replacing sycl::nd_item::barrier() with sycl::nd_item::barrier(sycl::access::fence_space::local_space) for better performance if there is no access to global memory.
  104 |   __syncthreads();
      |   ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/layernorm.h:110:67: warning: DPCT1098:406: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
  110 |         local_out[i] * s_[1] * __ldg(&((float *)gamma)[col_id]) + __ldg(&((float *)beta)[col_id]);
      |                                                                   ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/layernorm.h:121:3: warning: DPCT1065:407: Consider replacing sycl::nd_item::barrier() with sycl::nd_item::barrier(sycl::access::fence_space::local_space) for better performance if there is no access to global memory.
  121 |   __syncthreads();
      |   ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/layernorm.h:134:3: warning: DPCT1065:408: Consider replacing sycl::nd_item::barrier() with sycl::nd_item::barrier(sycl::access::fence_space::local_space) for better performance if there is no access to global memory.
  134 |   __syncthreads();
      |   ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/layernorm.h:141:22: warning: DPCT1098:409: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
  141 |       gamma_val[i] = __ldg(&((const float2 *)gamma)[col_id]);
      |                      ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/layernorm.h:142:21: warning: DPCT1098:410: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
  142 |       beta_val[i] = __ldg(&((const float2 *)beta)[col_id]);
      |                     ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/layernorm.h:148:37: warning: DPCT1098:411: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
  148 |       gamma_val[i] = __half22float2(__ldg(&((const half2 *)gamma)[col_id]));
      |                                     ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/layernorm.h:149:36: warning: DPCT1098:412: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
  149 |       beta_val[i] = __half22float2(__ldg(&((const half2 *)beta)[col_id]));
      |                                    ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/layernorm.h:170:20: warning: DPCT1098:413: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
  170 |     local_out[i] = __ldg(&input[id]);
      |                    ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/layernorm.h:189:39: warning: DPCT1098:414: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
  189 |     local_out_fp2[i] = __half22float2(__ldg(&input_ptr[id]));
      |                                       ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/layernorm.h:201:21: warning: DPCT1098:415: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
  201 |   int from_offset = __ldg(&word_idx[blockIdx.x]) * n;
      |                     ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/layernorm.h:210:20: warning: DPCT1098:416: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
  210 |     local_out[i] = __ldg(&input[id]);
      |                    ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/layernorm.h:224:21: warning: DPCT1098:417: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
  224 |   int from_offset = __ldg(&word_idx[blockIdx.x]) * n / 2;
      |                     ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/layernorm.h:232:18: warning: DPCT1098:418: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
  232 |     half2 temp = __ldg(&input_ptr[from_offset + col_id]);
      |                  ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/layernorm.h:265:58: warning: DPCT1098:419: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
  265 |     local_out[i] = (float)(out[id] + __ldg(&input[id]) + __ldg(&bias[col_id]));
      |                                                          ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/layernorm.h:290:38: warning: DPCT1098:420: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
  290 |         __hadd2(__hadd2(out_ptr[id], __ldg(&input_ptr[id])), __ldg(&bias_ptr[col_id])));
      |                                      ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/layernorm.h:307:28: warning: DPCT1098:421: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
  307 |   const int batch_offset = __ldg(&batch_idx[batch_id]);
      |                            ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/layernorm.h:308:29: warning: DPCT1098:422: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
  308 |   const int batch_seq_len = __ldg(&batch_idx[batch_id + 1]) - batch_offset;
      |                             ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/layernorm.h:326:50: warning: DPCT1098:423: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
  326 |     local_out[i] = (float)(out[id] + input[id] + __ldg(&bias[col_id]));
      |                                                  ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/layernorm.h:347:28: warning: DPCT1098:424: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
  347 |   const int batch_offset = __ldg(&batch_idx[batch_id]);
      |                            ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/layernorm.h:348:29: warning: DPCT1098:425: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
  348 |   const int batch_seq_len = __ldg(&batch_idx[batch_id + 1]) - batch_offset;
      |                             ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/layernorm.h:367:38: warning: DPCT1098:426: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
  367 |         __hadd2(__hadd2(out_ptr[id], __ldg(&input_ptr[id])), __ldg(&bias_ptr[col_id])));
      |                                      ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/layernorm.h:467:50: warning: DPCT1098:427: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
  467 |     local_out[i] = out[id] + __ldg(&input[id]) + __ldg(&bias[col_id]);
      |                                                  ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/layernorm.h:494:47: warning: DPCT1098:428: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
  494 |     half2 temp = __hadd2(__hadd2(out_ptr[id], __ldg(&input_ptr[id])), __ldg(&bias_ptr[col_id]));
      |                                               ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/layernorm.h:541:62: warning: DPCT1098:429: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
  541 |     local_out[i] = (out[id] + __ldg(&bias[col_id])) * 0.5f + __ldg(&input[id]);
      |                                                              ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/layernorm.h:565:46: warning: DPCT1098:430: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
  565 |         __hadd2(__hmul2(__hadd2(out_ptr[id], __ldg(&bias_ptr[col_id])), half2(0.5f, 0.5f)),
      |                                              ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/layernorm.h:566:17: warning: DPCT1098:431: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
  566 |                 __ldg(&input_ptr[id])));
      |                 ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/layernorm.h:586:28: warning: DPCT1098:432: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
  586 |   const int batch_offset = __ldg(&batch_idx[batch_id]);
      |                            ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/layernorm.h:587:29: warning: DPCT1098:433: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
  587 |   const int batch_seq_len = __ldg(&batch_idx[batch_id + 1]) - batch_offset;
      |                             ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/layernorm.h:605:62: warning: DPCT1098:434: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
  605 |     local_out[i] = (out[id] + __ldg(&bias[col_id])) * 0.5f + __ldg(&input[id]);
      |                                                              ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/layernorm.h:624:28: warning: DPCT1098:435: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
  624 |   const int batch_offset = __ldg(&batch_idx[batch_id]);
      |                            ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/layernorm.h:625:29: warning: DPCT1098:436: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
  625 |   const int batch_seq_len = __ldg(&batch_idx[batch_id + 1]) - batch_offset;
      |                             ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/layernorm.h:644:46: warning: DPCT1098:437: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
  644 |         __hadd2(__hmul2(__hadd2(out_ptr[id], __ldg(&bias_ptr[col_id])), half2(0.5f, 0.5f)),
      |                                              ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/layernorm.h:645:17: warning: DPCT1098:438: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
  645 |                 __ldg(&input_ptr[id])));
      |                 ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/layernorm.h:671:62: warning: DPCT1098:439: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
  671 |     local_out[i] = (out[id] + __ldg(&bias[col_id])) * 0.5f + __ldg(&input[id]);
      |                                                              ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/layernorm.h:698:46: warning: DPCT1098:440: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
  698 |         __hadd2(__hmul2(__hadd2(out_ptr[id], __ldg(&bias_ptr[col_id])), half2(0.5f, 0.5f)),
      |                                              ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/layernorm.h:699:17: warning: DPCT1098:441: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
  699 |                 __ldg(&input_ptr[id]));
      |                 ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/layernorm.cu:28:21: warning: DPCT1098:442: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
   28 |   float local_out = __ldg(&input[offset]);
      |                     ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/layernorm.cu:41:17: warning: DPCT1098:443: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
   41 |     local_out = __ldg(&((const half2 *)input)[offset]);
      |                 ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/layernorm.cu:52:20: warning: DPCT1098:444: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
   52 |   int src_offset = __ldg(&word_idx[blockIdx.x]) * n + threadIdx.x;
      |                    ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/layernorm.cu:55:21: warning: DPCT1098:445: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
   55 |   float local_out = __ldg(&input[src_offset]);
      |                     ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/layernorm.cu:67:20: warning: DPCT1098:446: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
   67 |   int src_offset = __ldg(&word_idx[blockIdx.x]) * n / 2 + threadIdx.x;
      |                    ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/layernorm.cu:70:21: warning: DPCT1098:447: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
   70 |   half2 local_out = __ldg(&((const half2 *)input)[src_offset]);
      |                     ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/layernorm.cu:83:67: warning: DPCT1098:448: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
   83 |   float local_out = (float)(out[offset] + __ldg(&input[offset]) + __ldg(&bias[threadIdx.x]));
      |                                                                   ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/layernorm.cu:97:57: warning: DPCT1098:449: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
   97 |     local_out = __hadd2(__hadd2(((half2 *)out)[offset], __ldg(&((const half2 *)input)[offset])),
      |                                                         ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/layernorm.cu:98:25: warning: DPCT1098:450: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
   98 |                         __ldg(&((const half2 *)bias)[threadIdx.x]));
      |                         ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/layernorm.cu:98:25: warning: DPCT1064:451: Migrated __ldg call is used in a macro/template definition and may not be valid for all macro/template uses. Adjust the code.
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/layernorm.cu:111:28: warning: DPCT1098:452: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
  111 |   const int batch_offset = __ldg(&batch_idx[batch_id]);
      |                            ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/layernorm.cu:112:29: warning: DPCT1098:453: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
  112 |   const int batch_seq_len = __ldg(&batch_idx[batch_id + 1]) - batch_offset;
      |                             ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/layernorm.cu:121:69: warning: DPCT1098:454: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
  121 |       (float)(__ldg(&out[src_offset]) + __ldg(&input[src_offset]) + __ldg(&bias[threadIdx.x]));
      |                                                                     ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/layernorm.cu:133:28: warning: DPCT1098:455: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
  133 |   const int batch_offset = __ldg(&batch_idx[batch_id]);
      |                            ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/layernorm.cu:134:29: warning: DPCT1098:456: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
  134 |   const int batch_seq_len = __ldg(&batch_idx[batch_id + 1]) - batch_offset;
      |                             ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/layernorm.cu:143:15: warning: DPCT1098:457: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
  143 |       __hadd2(__ldg(&((half2 *)out)[src_offset]), __ldg(&((const half2 *)input)[src_offset])),
      |               ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/layernorm.cu:144:7: warning: DPCT1098:458: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
  144 |       __ldg(&((const half2 *)bias)[threadIdx.x]));
      |       ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/layernorm.cu:144:7: warning: DPCT1064:459: Migrated __ldg call is used in a macro/template definition and may not be valid for all macro/template uses. Adjust the code.
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/layernorm.cu:158:59: warning: DPCT1098:460: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
  158 |   float local_out = out[offset] + __ldg(&input[offset]) + __ldg(&bias[threadIdx.x]);
      |                                                           ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/layernorm.cu:173:47: warning: DPCT1098:461: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
  173 |       __hadd2(__hadd2(((half2 *)out)[offset], __ldg(&((const half2 *)input)[offset])),
      |                                               ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/layernorm.cu:174:15: warning: DPCT1098:462: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
  174 |               __ldg(&((const half2 *)bias)[threadIdx.x]));
      |               ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/layernorm.cu:174:15: warning: DPCT1064:463: Migrated __ldg call is used in a macro/template definition and may not be valid for all macro/template uses. Adjust the code.
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/layernorm.cu:189:72: warning: DPCT1098:464: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
  189 |   float local_out = (out[offset] + __ldg(&bias[threadIdx.x])) * 0.5f + __ldg(&input[offset]);
      |                                                                        ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/layernorm.cu:202:55: warning: DPCT1098:465: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
  202 |       __hadd2(__hmul2(__hadd2(((half2 *)out)[offset], __ldg(&((const half2 *)bias)[threadIdx.x])),
      |                                                       ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/layernorm.cu:204:15: warning: DPCT1098:466: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
  204 |               __ldg(&((const half2 *)input)[offset]));
      |               ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/layernorm.cu:202:55: warning: DPCT1064:467: Migrated __ldg call is used in a macro/template definition and may not be valid for all macro/template uses. Adjust the code.
  202 |       __hadd2(__hmul2(__hadd2(((half2 *)out)[offset], __ldg(&((const half2 *)bias)[threadIdx.x])),
      |                                                       ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/layernorm.cu:202:23: warning: DPCT1064:468: Migrated __hadd2 call is used in a macro/template definition and may not be valid for all macro/template uses. Adjust the code.
  202 |       __hadd2(__hmul2(__hadd2(((half2 *)out)[offset], __ldg(&((const half2 *)bias)[threadIdx.x])),
      |                       ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/layernorm.cu:202:15: warning: DPCT1064:469: Migrated __hmul2 call is used in a macro/template definition and may not be valid for all macro/template uses. Adjust the code.
  202 |       __hadd2(__hmul2(__hadd2(((half2 *)out)[offset], __ldg(&((const half2 *)bias)[threadIdx.x])),
      |               ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/layernorm.cu:217:28: warning: DPCT1098:470: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
  217 |   const int batch_offset = __ldg(&batch_idx[batch_id]);
      |                            ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/layernorm.cu:218:29: warning: DPCT1098:471: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
  218 |   const int batch_seq_len = __ldg(&batch_idx[batch_id + 1]) - batch_offset;
      |                             ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/layernorm.cu:227:70: warning: DPCT1098:472: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
  227 |       (__ldg(&out[src_offset]) + __ldg(&bias[threadIdx.x])) * 0.5f + __ldg(&input[src_offset]);
      |                                                                      ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/layernorm.cu:240:28: warning: DPCT1098:473: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
  240 |   const int batch_offset = __ldg(&batch_idx[batch_id]);
      |                            ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/layernorm.cu:241:29: warning: DPCT1098:474: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
  241 |   const int batch_seq_len = __ldg(&batch_idx[batch_id + 1]) - batch_offset;
      |                             ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/layernorm.cu:249:45: warning: DPCT1098:475: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
  249 |   half2 local_out = __hadd2(__hmul2(__hadd2(__ldg(&((half2 *)out)[src_offset]),
      |                                             ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/layernorm.cu:250:45: warning: DPCT1098:476: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
  250 |                                             __ldg(&((const half2 *)bias)[threadIdx.x])),
      |                                             ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/layernorm.cu:252:29: warning: DPCT1098:477: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
  252 |                             __ldg(&((const half2 *)input)[src_offset]));
      |                             ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/layernorm.cu:250:45: warning: DPCT1064:478: Migrated __ldg call is used in a macro/template definition and may not be valid for all macro/template uses. Adjust the code.
  250 |                                             __ldg(&((const half2 *)bias)[threadIdx.x])),
      |                                             ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/layernorm.cu:249:37: warning: DPCT1064:479: Migrated __hadd2 call is used in a macro/template definition and may not be valid for all macro/template uses. Adjust the code.
  249 |   half2 local_out = __hadd2(__hmul2(__hadd2(__ldg(&((half2 *)out)[src_offset]),
      |                                     ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/layernorm.cu:249:29: warning: DPCT1064:480: Migrated __hmul2 call is used in a macro/template definition and may not be valid for all macro/template uses. Adjust the code.
  249 |   half2 local_out = __hadd2(__hmul2(__hadd2(__ldg(&((half2 *)out)[src_offset]),
      |                             ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/layernorm.cu:265:72: warning: DPCT1098:481: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
  265 |   float local_out = (out[offset] + __ldg(&bias[threadIdx.x])) * 0.5f + __ldg(&input[offset]);
      |                                                                        ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/layernorm.cu:280:55: warning: DPCT1098:482: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
  280 |       __hadd2(__hmul2(__hadd2(((half2 *)out)[offset], __ldg(&((const half2 *)bias)[threadIdx.x])),
      |                                                       ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/layernorm.cu:282:15: warning: DPCT1098:483: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
  282 |               __ldg(&((const half2 *)input)[offset]));
      |               ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/layernorm.cu:280:55: warning: DPCT1064:484: Migrated __ldg call is used in a macro/template definition and may not be valid for all macro/template uses. Adjust the code.
  280 |       __hadd2(__hmul2(__hadd2(((half2 *)out)[offset], __ldg(&((const half2 *)bias)[threadIdx.x])),
      |                                                       ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/layernorm.cu:280:23: warning: DPCT1064:485: Migrated __hadd2 call is used in a macro/template definition and may not be valid for all macro/template uses. Adjust the code.
  280 |       __hadd2(__hmul2(__hadd2(((half2 *)out)[offset], __ldg(&((const half2 *)bias)[threadIdx.x])),
      |                       ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/layernorm.cu:280:15: warning: DPCT1064:486: Migrated __hmul2 call is used in a macro/template definition and may not be valid for all macro/template uses. Adjust the code.
  280 |       __hadd2(__hmul2(__hadd2(((half2 *)out)[offset], __ldg(&((const half2 *)bias)[threadIdx.x])),
      |               ^
In file included from /export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/remove_padding.cu:20:
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/remove_padding.h:59:3: warning: DPCT1065:487: Consider replacing sycl::nd_item::barrier() with sycl::nd_item::barrier(sycl::access::fence_space::local_space) for better performance if there is no access to global memory.
   59 |   __syncthreads();
      |   ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/remove_padding.h:76:3: warning: DPCT1065:488: Consider replacing sycl::nd_item::barrier() with sycl::nd_item::barrier(sycl::access::fence_space::local_space) for better performance if there is no access to global memory.
   76 |   __syncthreads();
      |   ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/remove_padding.h:107:16: warning: DPCT1098:489: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
  107 |   int offset = __ldg(&word_idx[blockIdx.x]);
      |                ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/remove_padding.cu:28:54: warning: DPCT1098:490: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
   28 |     out[index] = out[index] + __ldg(&input[index]) + __ldg(&bias[i]);
      |                                                      ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/remove_padding.cu:41:36: warning: DPCT1098:491: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
   41 |       __hadd2(__hadd2(out_ptr[id], __ldg(&input_ptr[id])), __ldg(&bias_ptr[threadIdx.x]));
      |                                    ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/remove_padding.cu:41:60: warning: DPCT1064:492: Migrated __ldg call is used in a macro/template definition and may not be valid for all macro/template uses. Adjust the code.
   41 |       __hadd2(__hadd2(out_ptr[id], __ldg(&input_ptr[id])), __ldg(&bias_ptr[threadIdx.x]));
      |                                                            ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/remove_padding.cu:51:28: warning: DPCT1098:493: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
   51 |   const int batch_offset = __ldg(&batch_idx[batch_id]);
      |                            ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/remove_padding.cu:52:29: warning: DPCT1098:494: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
   52 |   const int batch_seq_len = __ldg(&batch_idx[batch_id + 1]) - batch_offset;
      |                             ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/remove_padding.cu:63:64: warning: DPCT1098:495: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
   63 |     out2[dst_offset + i] = out[index] + __ldg(&input[index]) + __ldg(&bias[i]);
      |                                                                ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/remove_padding.cu:78:28: warning: DPCT1098:496: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
   78 |   const int batch_offset = __ldg(&batch_idx[batch_id]);
      |                            ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/remove_padding.cu:79:29: warning: DPCT1098:497: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
   79 |   const int batch_seq_len = __ldg(&batch_idx[batch_id + 1]) - batch_offset;
      |                             ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/remove_padding.cu:88:36: warning: DPCT1098:498: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
   88 |       __hadd2(out_ptr[src_offset], __ldg(&input_ptr[src_offset])), __ldg(&bias_ptr[threadIdx.x]));
      |                                    ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/remove_padding.cu:88:68: warning: DPCT1064:499: Migrated __ldg call is used in a macro/template definition and may not be valid for all macro/template uses. Adjust the code.
   88 |       __hadd2(out_ptr[src_offset], __ldg(&input_ptr[src_offset])), __ldg(&bias_ptr[threadIdx.x]));
      |                                                                    ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/remove_padding.cu:97:18: warning: DPCT1098:500: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
   97 |     out[index] = __ldg(&input[index]) + (out[index] + __ldg(&bias[i])) * 0.5f;
      |                  ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/remove_padding.cu:110:15: warning: DPCT1098:501: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
  110 |       __hadd2(__ldg(&input_ptr[id]),
      |               ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/remove_padding.cu:111:44: warning: DPCT1098:502: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
  111 |               __hmul2(__hadd2(out_ptr[id], __ldg(&bias_ptr[threadIdx.x])), half2(0.5f, 0.5f)));
      |                                            ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/remove_padding.cu:111:44: warning: DPCT1064:503: Migrated __ldg call is used in a macro/template definition and may not be valid for all macro/template uses. Adjust the code.
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/remove_padding.cu:111:23: warning: DPCT1064:504: Migrated __hadd2 call is used in a macro/template definition and may not be valid for all macro/template uses. Adjust the code.
  111 |               __hmul2(__hadd2(out_ptr[id], __ldg(&bias_ptr[threadIdx.x])), half2(0.5f, 0.5f)));
      |                       ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/remove_padding.cu:111:15: warning: DPCT1064:505: Migrated __hmul2 call is used in a macro/template definition and may not be valid for all macro/template uses. Adjust the code.
  111 |               __hmul2(__hadd2(out_ptr[id], __ldg(&bias_ptr[threadIdx.x])), half2(0.5f, 0.5f)));
      |               ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/remove_padding.cu:121:28: warning: DPCT1098:506: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
  121 |   const int batch_offset = __ldg(&batch_idx[batch_id]);
      |                            ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/remove_padding.cu:122:29: warning: DPCT1098:507: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
  122 |   const int batch_seq_len = __ldg(&batch_idx[batch_id + 1]) - batch_offset;
      |                             ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/remove_padding.cu:133:28: warning: DPCT1098:508: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
  133 |     out2[dst_offset + i] = __ldg(&input[index]) + (__ldg(&out[index]) + __ldg(&bias[i])) * 0.5f;
      |                            ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/remove_padding.cu:149:28: warning: DPCT1098:509: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
  149 |   const int batch_offset = __ldg(&batch_idx[batch_id]);
      |                            ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/remove_padding.cu:150:29: warning: DPCT1098:510: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
  150 |   const int batch_seq_len = __ldg(&batch_idx[batch_id + 1]) - batch_offset;
      |                             ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/remove_padding.cu:159:15: warning: DPCT1098:511: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
  159 |       __hadd2(__ldg(&input_ptr[src_offset]),
      |               ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/remove_padding.cu:160:31: warning: DPCT1098:512: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
  160 |               __hmul2(__hadd2(__ldg(&out_ptr[src_offset]), __ldg(&bias_ptr[threadIdx.x])),
      |                               ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/remove_padding.cu:160:60: warning: DPCT1064:513: Migrated __ldg call is used in a macro/template definition and may not be valid for all macro/template uses. Adjust the code.
  160 |               __hmul2(__hadd2(__ldg(&out_ptr[src_offset]), __ldg(&bias_ptr[threadIdx.x])),
      |                                                            ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/remove_padding.cu:160:23: warning: DPCT1064:514: Migrated __hadd2 call is used in a macro/template definition and may not be valid for all macro/template uses. Adjust the code.
  160 |               __hmul2(__hadd2(__ldg(&out_ptr[src_offset]), __ldg(&bias_ptr[threadIdx.x])),
      |                       ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/remove_padding.cu:160:15: warning: DPCT1064:515: Migrated __hmul2 call is used in a macro/template definition and may not be valid for all macro/template uses. Adjust the code.
  160 |               __hmul2(__hadd2(__ldg(&out_ptr[src_offset]), __ldg(&bias_ptr[threadIdx.x])),
      |               ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused.cu:57:18: warning: DPCT1098:516: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
   57 |   half2 q_bias = __ldg(&qkv_bias[quart_thread_offset]);
      |                  ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused.cu:58:18: warning: DPCT1098:517: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
   58 |   half2 k_bias = __ldg(&qkv_bias[quart_thread_offset + half_hidden_dim]);
      |                  ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused.cu:62:47: warning: DPCT1098:518: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
   62 |     *(__half2 *)(*s_query + offset) = __hadd2(__ldg(&qkv[pos]), q_bias);
      |                                               ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused.cu:63:44: warning: DPCT1098:519: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
   63 |     *(__half2 *)(*s_kv + offset) = __hadd2(__ldg(&qkv[pos + half_hidden_dim]), k_bias);
      |                                            ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused.cu:128:18: warning: DPCT1098:520: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
  128 |   half2 v_bias = __ldg(&qkv_bias[quart_thread_offset + half_hidden_dim * 2]);
      |                  ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused.cu:132:17: warning: DPCT1098:521: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
  132 |         __hadd2(__ldg(&qkv[pos + half_hidden_dim * 2]), v_bias);
      |                 ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused.cu:66:3: warning: DPCT1065:522: Consider replacing sycl::nd_item::barrier() with sycl::nd_item::barrier(sycl::access::fence_space::local_space) for better performance if there is no access to global memory.
   66 |   __syncthreads();
      |   ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused.cu:87:3: warning: DPCT1065:523: Consider replacing sycl::nd_item::barrier() with sycl::nd_item::barrier(sycl::access::fence_space::local_space) for better performance if there is no access to global memory.
   87 |   __syncthreads();
      |   ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused.cu:138:3: warning: DPCT1065:524: Consider replacing sycl::nd_item::barrier() with sycl::nd_item::barrier(sycl::access::fence_space::local_space) for better performance if there is no access to global memory.
  138 |   __syncthreads();
      |   ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused.cu:159:3: warning: DPCT1065:525: Consider replacing sycl::nd_item::barrier() with sycl::nd_item::barrier(sycl::access::fence_space::local_space) for better performance if there is no access to global memory.
  159 |   __syncthreads();
      |   ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused.cu:104:20: warning: DPCT1098:526: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
  104 |             (float)__ldg(&attention_mask[(batch_seq_offset + from_id) * seq_len + to_id[i]]);
      |                    ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused.cu:190:18: warning: DPCT1098:527: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
  190 |   half2 q_bias = __ldg(&qkv_bias[thread_offset]);
      |                  ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused.cu:191:18: warning: DPCT1098:528: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
  191 |   half2 k_bias = __ldg(&qkv_bias[thread_offset + half_hidden_dim]);
      |                  ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused.cu:195:47: warning: DPCT1098:529: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
  195 |     *(__half2 *)(*s_query + offset) = __hadd2(__ldg(&qkv[pos]), q_bias);
      |                                               ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused.cu:196:44: warning: DPCT1098:530: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
  196 |     *(__half2 *)(*s_kv + offset) = __hadd2(__ldg(&qkv[pos + half_hidden_dim]), k_bias);
      |                                            ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused.cu:221:18: warning: DPCT1098:531: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
  221 |   half2 v_bias = __ldg(&qkv_bias[thread_offset + half_hidden_dim * 2]);
      |                  ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused.cu:259:17: warning: DPCT1098:532: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
  259 |         __hadd2(__ldg(&qkv[pos + half_hidden_dim * 2]), v_bias);
      |                 ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused.cu:198:3: warning: DPCT1065:533: Consider replacing sycl::nd_item::barrier() with sycl::nd_item::barrier(sycl::access::fence_space::local_space) for better performance if there is no access to global memory.
  198 |   __syncthreads();
      |   ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused.cu:218:3: warning: DPCT1065:534: Consider replacing sycl::nd_item::barrier() with sycl::nd_item::barrier(sycl::access::fence_space::local_space) for better performance if there is no access to global memory.
  218 |   __syncthreads();
      |   ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused.cu:265:3: warning: DPCT1065:535: Consider replacing sycl::nd_item::barrier() with sycl::nd_item::barrier(sycl::access::fence_space::local_space) for better performance if there is no access to global memory.
  265 |   __syncthreads();
      |   ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused.cu:286:3: warning: DPCT1065:536: Consider replacing sycl::nd_item::barrier() with sycl::nd_item::barrier(sycl::access::fence_space::local_space) for better performance if there is no access to global memory.
  286 |   __syncthreads();
      |   ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused.cu:235:20: warning: DPCT1098:537: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
  235 |             (float)__ldg(&attention_mask[(batch_seq_offset + from_id) * seq_len + to_id[i]]);
      |                    ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused.cu:312:28: warning: DPCT1098:538: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
  312 |   const int batch_offset = __ldg(&batch_idx[blockIdx.y]);
      |                            ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused.cu:313:29: warning: DPCT1098:539: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
  313 |   const int batch_seq_len = __ldg(&batch_idx[blockIdx.y + 1]) - batch_offset;
      |                             ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused.cu:318:18: warning: DPCT1098:540: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
  318 |   half2 q_bias = __ldg(&qkv_bias[thread_offset]);
      |                  ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused.cu:319:18: warning: DPCT1098:541: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
  319 |   half2 k_bias = __ldg(&qkv_bias[thread_offset + half_hidden_dim]);
      |                  ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused.cu:323:47: warning: DPCT1098:542: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
  323 |     *(__half2 *)(*s_query + offset) = __hadd2(__ldg(&qkv[pos]), q_bias);
      |                                               ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused.cu:324:44: warning: DPCT1098:543: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
  324 |     *(__half2 *)(*s_kv + offset) = __hadd2(__ldg(&qkv[pos + half_hidden_dim]), k_bias);
      |                                            ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused.cu:349:18: warning: DPCT1098:544: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
  349 |   half2 v_bias = __ldg(&qkv_bias[thread_offset + half_hidden_dim * 2]);
      |                  ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused.cu:380:17: warning: DPCT1098:545: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
  380 |         __hadd2(__ldg(&qkv[pos + half_hidden_dim * 2]), v_bias);
      |                 ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused.cu:326:3: warning: DPCT1065:546: Consider replacing sycl::nd_item::barrier() with sycl::nd_item::barrier(sycl::access::fence_space::local_space) for better performance if there is no access to global memory.
  326 |   __syncthreads();
      |   ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused.cu:346:3: warning: DPCT1065:547: Consider replacing sycl::nd_item::barrier() with sycl::nd_item::barrier(sycl::access::fence_space::local_space) for better performance if there is no access to global memory.
  346 |   __syncthreads();
      |   ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused.cu:386:3: warning: DPCT1065:548: Consider replacing sycl::nd_item::barrier() with sycl::nd_item::barrier(sycl::access::fence_space::local_space) for better performance if there is no access to global memory.
  386 |   __syncthreads();
      |   ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused.cu:407:3: warning: DPCT1065:549: Consider replacing sycl::nd_item::barrier() with sycl::nd_item::barrier(sycl::access::fence_space::local_space) for better performance if there is no access to global memory.
  407 |   __syncthreads();
      |   ^
In file included from /export/users/zhaojiam/src/ByteTransformer/unit_test/bert_transformer_test.cc:20:
In file included from /export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/bert_transformer.h:21:
In file included from /export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/attention.h:22:
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/common.h:74:19: warning: DPCT1064:550: Migrated __expf call is used in a macro/template definition and may not be valid for all macro/template uses. Adjust the code.
   74 |     return __logf(__expf((float)val) + 1.0f);
      |                   ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/common.h:76:27: warning: DPCT1064:551: Migrated __expf call is used in a macro/template definition and may not be valid for all macro/template uses. Adjust the code.
   76 |     return 1.0f / (1.0f + __expf(-1.0f * (float)val));
      |                           ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/common.h:133:5: warning: DPCT1001:552: The statement could not be removed.
  133 |     throw std::runtime_error(std::string("[BT][ERROR] CUDA runtime error: ") +
      |     ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/common.h:132:3: warning: DPCT1000:553: Error handling if-stmt was detected but could not be rewritten.
  132 |   if (result)
      |   ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_nofused_utils.cu:35:54: warning: DPCT1098:554: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
   35 |   float2 q_value = ((float2 *)QKV)[src_id], q_bias = __ldg(&((float2 *)bias_QKV)[threadIdx.x]);
      |                                                      ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_nofused_utils.cu:37:19: warning: DPCT1098:555: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
   37 |          k_bias = __ldg(&((float2 *)bias_QKV)[threadIdx.x + blockDim.x]);
      |                   ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_nofused_utils.cu:39:19: warning: DPCT1098:556: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
   39 |          v_bias = __ldg(&((float2 *)bias_QKV)[threadIdx.x + blockDim.x * 2]);
      |                   ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_nofused_utils.cu:73:45: warning: DPCT1098:557: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
   73 |       __hadd2(((const half2 *)QKV)[src_id], __ldg(&((const half2 *)bias_QKV)[threadIdx.x]));
      |                                             ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_nofused_utils.cu:73:45: warning: DPCT1064:558: Migrated __ldg call is used in a macro/template definition and may not be valid for all macro/template uses. Adjust the code.
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_nofused_utils.cu:75:27: warning: DPCT1098:559: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
   75 |                           __ldg(&((const half2 *)bias_QKV)[threadIdx.x + blockDim.x]));
      |                           ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_nofused_utils.cu:75:27: warning: DPCT1064:560: Migrated __ldg call is used in a macro/template definition and may not be valid for all macro/template uses. Adjust the code.
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_nofused_utils.cu:77:27: warning: DPCT1098:561: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
   77 |                           __ldg(&((const half2 *)bias_QKV)[threadIdx.x + blockDim.x * 2]));
      |                           ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_nofused_utils.cu:77:27: warning: DPCT1064:562: Migrated __ldg call is used in a macro/template definition and may not be valid for all macro/template uses. Adjust the code.
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_nofused_utils.cu:105:28: warning: DPCT1098:563: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
  105 |   const int batch_offset = __ldg(&batch_idx[blockIdx.y]);
      |                            ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_nofused_utils.cu:106:29: warning: DPCT1098:564: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
  106 |   const int batch_seq_len = __ldg(&batch_idx[blockIdx.y + 1]) - batch_offset;
      |                             ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_nofused_utils.cu:109:56: warning: DPCT1098:565: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
  109 |     float2 q_value = ((float2 *)QKV)[src_id], q_bias = __ldg(&((float2 *)bias_QKV)[threadIdx.x]);
      |                                                        ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_nofused_utils.cu:111:21: warning: DPCT1098:566: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
  111 |            k_bias = __ldg(&((float2 *)bias_QKV)[threadIdx.x + blockDim.x]);
      |                     ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_nofused_utils.cu:113:21: warning: DPCT1098:567: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
  113 |            v_bias = __ldg(&((float2 *)bias_QKV)[threadIdx.x + blockDim.x * 2]);
      |                     ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_nofused_utils.cu:154:28: warning: DPCT1098:568: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
  154 |   const int batch_offset = __ldg(&batch_idx[blockIdx.y]);
      |                            ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_nofused_utils.cu:155:29: warning: DPCT1098:569: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
  155 |   const int batch_seq_len = __ldg(&batch_idx[blockIdx.y + 1]) - batch_offset;
      |                             ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_nofused_utils.cu:159:47: warning: DPCT1098:570: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
  159 |         __hadd2(((const half2 *)QKV)[src_id], __ldg(&((const half2 *)bias_QKV)[threadIdx.x]));
      |                                               ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_nofused_utils.cu:159:47: warning: DPCT1064:571: Migrated __ldg call is used in a macro/template definition and may not be valid for all macro/template uses. Adjust the code.
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_nofused_utils.cu:161:29: warning: DPCT1098:572: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
  161 |                             __ldg(&((const half2 *)bias_QKV)[threadIdx.x + blockDim.x]));
      |                             ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_nofused_utils.cu:161:29: warning: DPCT1064:573: Migrated __ldg call is used in a macro/template definition and may not be valid for all macro/template uses. Adjust the code.
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_nofused_utils.cu:163:29: warning: DPCT1098:574: The '*' expression is used instead of the __ldg call. These two expressions do not provide the exact same functionality. Check the generated code for potential precision and/or performance issues.
  163 |                             __ldg(&((const half2 *)bias_QKV)[threadIdx.x + blockDim.x * 2]));
      |                             ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_nofused_utils.cu:163:29: warning: DPCT1064:575: Migrated __ldg call is used in a macro/template definition and may not be valid for all macro/template uses. Adjust the code.
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/bert_transformer.cu:93:3: warning: DPCT1012:576: Detected kernel execution time measurement pattern and generated an initial code for time measurements in SYCL. You can change the way time is measured depending on your goals.
   93 |   cudaEventRecord(beg);
      |   ^
In file included from /export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/bert_transformer.cu:23:
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/layernorm.h:39:14: warning: DPCT1064:577: Migrated __ldg call is used in a macro/template definition and may not be valid for all macro/template uses. Adjust the code.
   39 |              __ldg(&((float *)beta)[threadIdx.x]);
      |              ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/layernorm.h:38:34: warning: DPCT1064:578: Migrated __ldg call is used in a macro/template definition and may not be valid for all macro/template uses. Adjust the code.
   38 |   *out_ptr = local_out * s_[1] * __ldg(&((float *)gamma)[threadIdx.x]) +
      |                                  ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/layernorm.h:64:19: warning: DPCT1064:579: Migrated __ldg call is used in a macro/template definition and may not be valid for all macro/template uses. Adjust the code.
   64 |       gamma_val = __ldg(&((const float2 *)gamma)[threadIdx.x]);
      |                   ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/layernorm.h:65:18: warning: DPCT1064:580: Migrated __ldg call is used in a macro/template definition and may not be valid for all macro/template uses. Adjust the code.
   65 |       beta_val = __ldg(&((const float2 *)beta)[threadIdx.x]);
      |                  ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/layernorm.h:67:19: warning: DPCT1064:581: Migrated __half22float2 call is used in a macro/template definition and may not be valid for all macro/template uses. Adjust the code.
   67 |       gamma_val = __half22float2(__ldg(&((const half2 *)gamma)[threadIdx.x]));
      |                   ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/layernorm.h:68:18: warning: DPCT1064:582: Migrated __half22float2 call is used in a macro/template definition and may not be valid for all macro/template uses. Adjust the code.
   68 |       beta_val = __half22float2(__ldg(&((const half2 *)beta)[threadIdx.x]));
      |                  ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/layernorm.h:201:21: warning: DPCT1064:583: Migrated __ldg call is used in a macro/template definition and may not be valid for all macro/template uses. Adjust the code.
  201 |   int from_offset = __ldg(&word_idx[blockIdx.x]) * n;
      |                     ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/layernorm.h:224:21: warning: DPCT1064:584: Migrated __ldg call is used in a macro/template definition and may not be valid for all macro/template uses. Adjust the code.
  224 |   int from_offset = __ldg(&word_idx[blockIdx.x]) * n / 2;
      |                     ^
In file included from /export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/bert_transformer.cu:24:
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/remove_padding.h:107:16: warning: DPCT1064:585: Migrated __ldg call is used in a macro/template definition and may not be valid for all macro/template uses. Adjust the code.
  107 |   int offset = __ldg(&word_idx[blockIdx.x]);
      |                ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/reduce.h:92:21: warning: DPCT1096:586: The right-most dimension of the work-group used in the SYCL kernel that calls this function may be less than "32". The function "dpct::permute_sub_group_by_xor" may return an unexpected result on the CPU device. Modify the size of the work-group to ensure that the value of the right-most dimension is a multiple of "32".
   92 |     val = max_(val, __shfl_xor_sync(FINAL_MASK, val, mask, 32));
      |                     ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/reduce.h:35:12: warning: DPCT1096:587: The right-most dimension of the work-group used in the SYCL kernel that calls this function may be less than "32". The function "dpct::permute_sub_group_by_xor" may return an unexpected result on the CPU device. Modify the size of the work-group to ensure that the value of the right-most dimension is a multiple of "32".
   35 |     val += __shfl_xor_sync(FINAL_MASK, val, mask, 32);
      |            ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/remove_padding.h:44:25: warning: DPCT1083:588: The size of local memory in the migrated code may be different from the original code. Check that the allocated memory size in the migrated code is correct.
   44 |   extern __shared__ int base[];
      |                         ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/remove_padding.h:70:14: warning: DPCT1096:589: The right-most dimension of the work-group used in the SYCL kernel that calls this function may be less than "32". The function "dpct::select_from_sub_group" may return an unexpected result on the CPU device. Modify the size of the work-group to ensure that the value of the right-most dimension is a multiple of "32".
   70 |       temp = __shfl_sync(0xffffffff, temp, 31);
      |              ^
/export/users/zhaojiam/src/ByteTransformer/bytetransformer/include/remove_padding.h:29:13: warning: DPCT1096:590: The right-most dimension of the work-group used in the SYCL kernel that calls this function may be less than "32". The function "dpct::shift_sub_group_right" may return an unexpected result on the CPU device. Modify the size of the work-group to ensure that the value of the right-most dimension is a multiple of "32".
   29 |     T val = __shfl_up_sync(0xffffffff, count, i);
      |             ^
Saved new version of /export/users/zhaojiam/src/ByteTransformer/sycl/include/attention.h.yaml file
Saved new version of /export/users/zhaojiam/src/ByteTransformer/sycl/include/attention_nofused_utils.h.yaml file
Saved new version of /export/users/zhaojiam/src/ByteTransformer/sycl/include/bert_transformer.h.yaml file
Saved new version of /export/users/zhaojiam/src/ByteTransformer/sycl/include/common.h.yaml file
Saved new version of /export/users/zhaojiam/src/ByteTransformer/sycl/include/cutlass_attention.h.yaml file
Saved new version of /export/users/zhaojiam/src/ByteTransformer/sycl/include/cutlass_attention_operators.h.yaml file
Saved new version of /export/users/zhaojiam/src/ByteTransformer/sycl/include/gemm.h.yaml file
Saved new version of /export/users/zhaojiam/src/ByteTransformer/sycl/include/gemm_bias_act.h.yaml file
Saved new version of /export/users/zhaojiam/src/ByteTransformer/sycl/include/layernorm.h.yaml file
Saved new version of /export/users/zhaojiam/src/ByteTransformer/sycl/include/reduce.h.yaml file
Saved new version of /export/users/zhaojiam/src/ByteTransformer/sycl/include/remove_padding.h.yaml file
Saved new version of /export/users/zhaojiam/src/ByteTransformer/sycl/include/softmax.h.yaml file
Saved new version of /export/users/zhaojiam/src/ByteTransformer/sycl/include/variety_attention_fused.h.yaml file
Saved new version of /export/users/zhaojiam/src/ByteTransformer/sycl/th_op/bert_transformer_ext.h.yaml file
Saved new version of /export/users/zhaojiam/src/ByteTransformer/sycl/th_op/ths_op.h.yaml file
Saved new version of /export/users/zhaojiam/src/ByteTransformer/sycl/th_op/traits.h.yaml file
Saved new version of /export/users/zhaojiam/src/ByteTransformer/sycl/th_op/util.h.yaml file

Parsing: /export/users/zhaojiam/src/ByteTransformer/bytetransformer/th_op/ths_op_f.cc
Parsing: /export/users/zhaojiam/src/ByteTransformer/bytetransformer/th_op/ths_op.cc
Parsing: /export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused_long.cu
Parsing: /export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/gemm.cu
Parsing: /export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/gemm_bias_act.cu
Parsing: /export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_nofused.cu
Parsing: /export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/cutlass_attention.cu
Parsing: /export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/layernorm.cu
Parsing: /export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/remove_padding.cu
Parsing: /export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused.cu
Parsing: /export/users/zhaojiam/src/ByteTransformer/unit_test/bert_transformer_test.cc
Parsing: /export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_nofused_utils.cu
Parsing: /export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/bert_transformer.cu
Analyzing: /export/users/zhaojiam/src/ByteTransformer/bytetransformer/th_op/ths_op_f.cc
Analyzing: /export/users/zhaojiam/src/ByteTransformer/bytetransformer/th_op/ths_op.cc
Analyzing: /export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused_long.cu
Analyzing: /export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/gemm.cu
Analyzing: /export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/gemm_bias_act.cu
Analyzing: /export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_nofused.cu
Analyzing: /export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/cutlass_attention.cu
Analyzing: /export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/layernorm.cu
Analyzing: /export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/remove_padding.cu
Analyzing: /export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused.cu
Analyzing: /export/users/zhaojiam/src/ByteTransformer/unit_test/bert_transformer_test.cc
Analyzing: /export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_nofused_utils.cu
Analyzing: /export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/bert_transformer.cu
Migrating: /export/users/zhaojiam/src/ByteTransformer/bytetransformer/th_op/ths_op_f.cc
Migrating: /export/users/zhaojiam/src/ByteTransformer/bytetransformer/th_op/ths_op.cc
Migrating: /export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused_long.cu
Migrating: /export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/gemm.cu
Migrating: /export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/gemm_bias_act.cu
Migrating: /export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_nofused.cu
Migrating: /export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/cutlass_attention.cu
Migrating: /export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/layernorm.cu
Migrating: /export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/remove_padding.cu
Migrating: /export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_fused.cu
Migrating: /export/users/zhaojiam/src/ByteTransformer/unit_test/bert_transformer_test.cc
Migrating: /export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/attention_nofused_utils.cu
Migrating: /export/users/zhaojiam/src/ByteTransformer/bytetransformer/src/bert_transformer.cu
Processed 28 file(s) in -in-root folder "/export/users/zhaojiam/src/ByteTransformer/bytetransformer"

See Diagnostics Reference to resolve warnings and complete the migration:
https://www.intel.com/content/www/us/en/docs/dpcpp-compatibility-tool/developer-guide-reference/current/diagnostics-reference.html
